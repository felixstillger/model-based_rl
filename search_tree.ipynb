{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper\n",
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper_multi_instances\n",
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper_no_action_mask\n",
    "from src.jss_lite.jss_lite import jss_lite\n",
    "import numpy as np\n",
    "import time\n",
    "class tree_node:\n",
    "    def __init__(self, action=None,parent=None):\n",
    "        self.action=action\n",
    "        self.children=[]\n",
    "        self.state=None\n",
    "        self.parent=parent\n",
    "        self.observation=None\n",
    "        self.visited=False\n",
    "        # extra attributes for search, not used till now:\n",
    "        self.leaf_closed=False\n",
    "        \n",
    "class tree_construct:\n",
    "    def __init__(self):\n",
    "        # change if you want to load klagenfurt ...\n",
    "        self.load_klagen=False\n",
    "        # threshold for values that are as good as the threshold*max_policy\n",
    "        self.threshold=0.7\n",
    "        curr_dir='/Users/felix/sciebo/masterarbeit/progra/model-based_rl'\n",
    "        inst=curr_dir + '/resources/jsp_instances/standard/la02.txt'\n",
    "        instance_list=['/resources/jsp_instances/standard/la01.txt']#,'/resources/jsp_instances/standard/la02.txt','/resources/jsp_instances/standard/la03.txt','/resources/jsp_instances/standard/la04.txt']\n",
    "        #self.env=jssp_light_obs_wrapper_multi_instances(instances_list=[inst])\n",
    "        \n",
    "        from wrapper.jssp_wrapper_klagenfurt import jssp_klagenfurt_obs_wrapper\n",
    "        import JSSEnv\n",
    "        import gym\n",
    "        self.env=jssp_light_obs_wrapper_no_action_mask(jssp_light_obs_wrapper_multi_instances(instances_list=[inst]))\n",
    "\n",
    "        # create root\n",
    "        self.root=tree_node()\n",
    "        # get initial state into root; todo: change for multi instances, create all childs for instances\n",
    "        self.root.observation=self.env.reset()\n",
    "        self.root.state=self.env.get_state()\n",
    "        self.done=False\n",
    "        self.reward=0\n",
    "        self.trainer=self.load_trainer()\n",
    "        # path management\n",
    "        # open leafs should store the leafs with highest rating\n",
    "        self.open_leafs=[]\n",
    "        self.open_leafs.append(self.root)\n",
    "        self.current_path=self.root\n",
    "        self.sol_pathes=None\n",
    "    def load_environment(self,path):\n",
    "        self.env=jssp_light_obs_wrapper_no_action_mask(jssp_light_obs_wrapper_multi_instances(instances_list=[path]))\n",
    "        self.reset()\n",
    "    def insert(self,action=None,parent=None):\n",
    "        parent.children.append(tree_node(action=action,parent=self))\n",
    "        self.env.set_state(parent.state)\n",
    "        parent.children[-1].observation,reward,done,info=self.env.step(action)\n",
    "        parent.children[-1].state=self.env.get_state()\n",
    "        if done:\n",
    "            #this goes for klagenfurt jssp\n",
    "            self.__on_done(reward)\n",
    "            \n",
    "    def reset(self):\n",
    "        self.root=tree_node()\n",
    "        # get initial state into root; todo: change for multi instances, create all childs for instances\n",
    "        self.root.observation=self.env.reset()\n",
    "        self.root.state=self.env.get_state()\n",
    "        self.done=False\n",
    "        self.reward=0\n",
    "        #self.trainer=self.load_trainer()\n",
    "        # path management\n",
    "        # open leafs should store the leafs with highest rating\n",
    "        self.open_leafs=[]\n",
    "        self.open_leafs.append(self.root)\n",
    "        self.current_path=self.root\n",
    "        self.sol_pathes=None\n",
    "\n",
    "    def __on_done(self,reward):\n",
    "        self.done=True\n",
    "        if reward > self.reward:\n",
    "            self.reward=reward\n",
    "    def load_checkpoint(self,path):\n",
    "        self.trainer.load_checkpoint(path)\n",
    "    def load_trainer(self):\n",
    "        from ray.rllib.agents.ppo import PPOTrainer\n",
    "        from ray import tune\n",
    "        import gym\n",
    "        from wrapper.jssplight_wrapper import jssp_light_obs_wrapper\n",
    "        from wrapper.jssplight_wrapper import jssp_light_obs_wrapper_multi_instances\n",
    "        from wrapper.jssplight_wrapper import jssp_light_obs_wrapper_no_action_mask\n",
    "        from src.jss_lite.jss_lite import jss_lite\n",
    "        from ray import tune\n",
    "\n",
    "        if self.load_klagen:\n",
    "            from wrapper.jssp_wrapper_klagenfurt import jssp_klagenfurt_obs_wrapper\n",
    "            import JSSEnv\n",
    "            #Configure the algorithm.\n",
    "            def env_creator(env_config):\n",
    "                import JSSEnv\n",
    "                env = jssp_light_obs_wrapper_no_action_mask(jssp_klagenfurt_obs_wrapper(gym.make('jss-v1', env_config={'instance_path': 'resources/jsp_instances/standard/la01.txt'})))\n",
    "                return env\n",
    "        else:\n",
    "            curr_dir='/Users/felix/sciebo/masterarbeit/progra/model-based_rl'\n",
    "            inst=curr_dir + '/resources/jsp_instances/standard/la01.txt'\n",
    "            instance_list=['/resources/jsp_instances/standard/la01.txt']#,'/resources/jsp_instances/standard/la02.txt','/resources/jsp_instances/standard/la03.txt','/resources/jsp_instances/standard/la04.txt']\n",
    "            def env_creator(env_config):\n",
    "                #env=jssp_light_obs_wrapper_multi_instances(instances_list=[inst])\n",
    "                env=jssp_light_obs_wrapper_no_action_mask(jssp_light_obs_wrapper_multi_instances(instances_list=[inst]))\n",
    "                #env=jss_lite(instance_path=inst)\n",
    "                #env=jssp_light_obs_wrapper_no_action_mask(jss_lite(instance_path=inst))\n",
    "                return env\n",
    "\n",
    "        tune.register_env('custom_jssp',env_creator)\n",
    "        config = {\n",
    "            \"env\": \"custom_jssp\",\n",
    "            \"disable_env_checking\":True,\n",
    "            \"num_workers\": 4,\n",
    "            \"framework\": \"tf\",\n",
    "            \"model\": {\n",
    "                \"fcnet_hiddens\": [128, 128],\n",
    "                \"fcnet_activation\": \"relu\",\n",
    "            },\n",
    "            \"evaluation_num_workers\": 0,\n",
    "            \"evaluation_config\": {\n",
    "                \"render_env\": False,\n",
    "            },\n",
    "            \n",
    "        }\n",
    "        trainer = PPOTrainer(config=config)\n",
    "        \n",
    "        if self.load_klagen:\n",
    "            trainer.load_checkpoint('/Users/felix/sciebo/masterarbeit/progra/model-based_rl/training_checkpoints/klagen_ppo/checkpoint-60')\n",
    "        else:\n",
    "            trainer.load_checkpoint('/Users/felix/sciebo/masterarbeit/progra/model-based_rl/training_checkpoints/oneenvironment_punish2ima_8_8_ppo/checkpoint-194')\n",
    "        return trainer\n",
    "\n",
    "    def propose_actions(self,state,observation,threshold=None):\n",
    "        if threshold !=None:\n",
    "            self.threshold=threshold\n",
    "        policy = self.trainer.get_policy()\n",
    "        action, _, info = policy.compute_single_action(observation)\n",
    "        logits = info['action_dist_inputs']\n",
    "        self.env.set_state(state=state)\n",
    "        action_mask = self.env.get_legal_actions().astype(np.float32)\n",
    "        action_mask[action_mask==0] = - np.inf\n",
    "        probs=np.multiply(logits,action_mask)\n",
    "        probs[probs==np.inf]=-np.inf\n",
    "        best_action_id = probs.argmax()  # deterministic\n",
    "        highest_prob=probs.max()\n",
    "        #todo: check if it is good here\n",
    "        if highest_prob>0:\n",
    "            out,=np.where(probs>self.threshold*highest_prob)\n",
    "        else:\n",
    "            out,=np.where(probs>highest_prob/self.threshold)\n",
    "\n",
    "        return out, best_action_id    \n",
    "\n",
    "\n",
    "    def solve(self,max_pathes=None,max_time=None,s_threshold=None):\n",
    "        # here comes the solving method with abort conditions\n",
    "        start_time=time.time()\n",
    "        p_time=time.time()\n",
    "        self.sol_pathes=0\n",
    "\n",
    "        if max_pathes==None:\n",
    "            max_pathes=2 \n",
    "        if max_time==None:\n",
    "            max_time=20\n",
    "        #print(self.open_leafs)\n",
    "        while self.open_leafs:\n",
    "            self.sol_pathes+=1\n",
    "            # abort conditions:\n",
    "            node=self.open_leafs.pop()\n",
    "            if self.sol_pathes > max_pathes or time.time()-start_time > max_time or self.reward==1:\n",
    "                return True\n",
    "            while not self.done:\n",
    "                loop=0\n",
    "                \n",
    "                #possible_actions=np.random.choice(len(node.observation['action_mask']), p_r, p=(node.observation['action_mask'] / node.observation['action_mask'].sum()))\n",
    "                possible_actions, best_action = self.propose_actions(node.state,node.observation,threshold=s_threshold)\n",
    "                for action in set(possible_actions):\n",
    "                    if action!=best_action:\n",
    "                        #print(f\"action is: {action}\")\n",
    "                        self.insert(action=action,parent=node)\n",
    "                #assure that best action is insertet last\n",
    "                self.insert(action=best_action,parent=node)\n",
    "                for child in node.children:\n",
    "                    if child != node.children[-1]:\n",
    "                        #todo: check here if in action mask\n",
    "                        self.open_leafs.append(child)\n",
    "                while True:\n",
    "                    if node.children[-1].visited==False:\n",
    "                        node.children[-1].visited=True\n",
    "                        node=node.children[-1]\n",
    "                        \n",
    "                        break\n",
    "                    loop+=1\n",
    "                    if loop > len(node.children):\n",
    "                        print(\"all childrens visited; return done equal true\")\n",
    "                        self.done=True\n",
    "            self.done=False\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-31 17:52:00,579\tINFO trainable.py:159 -- Trainable.setup took 24.280 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solve\n",
      "50.150150150150154\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "ray.shutdown()\n",
    "init=tree_construct()\n",
    "print(\"solve\")\n",
    "init.solve()\n",
    "print(init.reward)\n",
    "print(init.sol_pathes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.76876876876877\n",
      "1001\n"
     ]
    }
   ],
   "source": [
    "init.reset()\n",
    "init.solve(max_pathes=1,max_time=30,s_threshold=0.9)\n",
    "print(init.reward)\n",
    "print(init.sol_pathes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-31 18:38:38,629\tINFO trainable.py:159 -- Trainable.setup took 24.188 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "ray.shutdown()\n",
    "init=tree_construct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/felix/sciebo/masterarbeit/progra/model-based_rl/training_checkpoints/oneenvironment_punish_v4_t_tima_3_3_ppo/checkpoint-150\n",
      "/Users/felix/sciebo/masterarbeit/progra/model-based_rl/training_checkpoints/oneenvironment_punish_v4_t_tima_6_6_ppo/checkpoint-150\n",
      "/Users/felix/sciebo/masterarbeit/progra/model-based_rl/training_checkpoints/oneenvironment_punish_v4_t_tima_8_8_ppo/checkpoint-150\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sizes=[3,6,8]\n",
    "curr_dir='/Users/felix/sciebo/masterarbeit/progra/model-based_rl'\n",
    "\n",
    "#times={'3':,'6','8':,'10':,'15':}\n",
    "#checkpoints=['/Users/felix/sciebo/masterarbeit/progra/model-based_rl/training_checkpoints/oneenvironment_punish_v4_t_tima_'+i+'_'+i+'_ppo' for i in sizes]\n",
    "for num_inst in sizes:\n",
    "    results={}\n",
    "    num_inst=str(num_inst)\n",
    "    checkpoint='/Users/felix/sciebo/masterarbeit/progra/model-based_rl/training_checkpoints/oneenvironment_punish_v4_t_tima_'+num_inst+'_'+num_inst+'_ppo/checkpoint-150' \n",
    "    init.load_checkpoint(checkpoint)\n",
    "    print(checkpoint)\n",
    "    test_instances=[]\n",
    "    for i in range(20,40):\n",
    "        test_instances.append(curr_dir+'/resources/jsp_instances/ima/'+num_inst+'x'+num_inst+'x'+num_inst+'/'+num_inst+'x'+num_inst+'_'+str(i)+'_inst.json')\n",
    "    for nr_i,inst in enumerate(test_instances):\n",
    "        #print(init.env.env.env.instance)\n",
    "        init.reset()\n",
    "        init.load_environment(inst)\n",
    "        init.solve(max_pathes=10000000,max_time=30,s_threshold=0.5)\n",
    "        results['instance_'+str(nr_i)]={'reward':init.reward,'number_pathes':init.sol_pathes}\n",
    "        #print(init.reward)\n",
    "        #print(init.sol_pathes)\n",
    "    df=pd.DataFrame.from_dict(results)\n",
    "    df.to_csv(f\"eval_tree_search_{num_inst}_inst_20_40_30sec_t05.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-27 12:18:59,371\tINFO trainable.py:159 -- Trainable.setup took 14.187 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray import tune\n",
    "import ray\n",
    "import gym\n",
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper\n",
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper_multi_instances\n",
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper_no_action_mask\n",
    "from src.jss_lite.jss_lite import jss_lite\n",
    "from ray import tune\n",
    "curr_dir='/Users/felix/sciebo/masterarbeit/progra/model-based_rl'\n",
    "inst=curr_dir + '/resources/jsp_instances/standard/la01.txt'\n",
    "instance_list=['/resources/jsp_instances/standard/la01.txt']#,'/resources/jsp_instances/standard/la02.txt','/resources/jsp_instances/standard/la03.txt','/resources/jsp_instances/standard/la04.txt']\n",
    "# Configure the algorithm.\n",
    "def env_creator(env_config):\n",
    "    #env=jssp_light_obs_wrapper_multi_instances(instances_list=[inst])\n",
    "    env=jssp_light_obs_wrapper_no_action_mask(jssp_light_obs_wrapper_multi_instances(instances_list=[inst]))\n",
    "    #env=jss_lite(instance_path=inst)\n",
    "    #env=jssp_light_obs_wrapper_no_action_mask(jss_lite(instance_path=inst))\n",
    "    return env\n",
    "from wrapper.jssp_wrapper_klagenfurt import jssp_klagenfurt_obs_wrapper\n",
    "import JSSEnv\n",
    "# #Configure the algorithm.\n",
    "# def env_creator(env_config):\n",
    "#     import JSSEnv\n",
    "#     env = jssp_light_obs_wrapper_no_action_mask(jssp_klagenfurt_obs_wrapper(gym.make('jss-v1', env_config={'instance_path': 'resources/jsp_instances/standard/la01.txt'})))\n",
    "#     return env\n",
    "\n",
    "ray.shutdown()\n",
    "# use tune to register the custom environment for the ppo trainer\n",
    "tune.register_env('custom_jssp',env_creator)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"env\": \"custom_jssp\",\n",
    "    \"disable_env_checking\":True,\n",
    "    \"num_workers\": 4,\n",
    "    \"framework\": \"tf\",\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [256, 256],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    \"evaluation_num_workers\": 0,\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": False,\n",
    "    },\n",
    "    \n",
    "}\n",
    "trainer = PPOTrainer(config=config)\n",
    "#trainer.load_checkpoint('/Users/felix/sciebo/masterarbeit/progra/model-based_rl/ppo/checkpoint-4')\n",
    "#trainer.load_checkpoint('/Users/felix/sciebo/masterarbeit/progra/model-based_rl/training_checkpoints/klagen_ppo/checkpoint-50')\n",
    "\n",
    "for _ in range(60):\n",
    "    print(_)\n",
    "    trainer.train()\n",
    "    #trainer.save_checkpoint(\"training_checkpoints/klagen_ppo\")\n",
    "    trainer.save_checkpoint(\"training_checkpoints/custom_ppo\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-30 17:57:24,145\tINFO trainable.py:159 -- Trainable.setup took 22.246 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in iteration1 reward is: 0.6818181818181819\n",
      "in iteration2 reward is: 0.8636363636363636\n",
      "in iteration3 reward is: 1.0\n",
      "in iteration4 reward is: 0.6818181818181819\n",
      "in iteration5 reward is: 0.6818181818181819\n",
      "in iteration6 reward is: 0.9090909090909091\n",
      "in iteration7 reward is: 0.6818181818181819\n",
      "in iteration8 reward is: 0.6818181818181819\n",
      "in iteration9 reward is: 0.6818181818181819\n",
      "in iteration10 reward is: 0.9090909090909091\n",
      "in iteration11 reward is: 1.0\n",
      "in iteration12 reward is: 1.0\n",
      "in iteration13 reward is: 1.0\n",
      "in iteration14 reward is: 1.0\n",
      "in iteration15 reward is: 1.0\n",
      "in iteration16 reward is: 1.0\n",
      "in iteration17 reward is: 1.0\n",
      "in iteration18 reward is: 1.0\n",
      "in iteration19 reward is: 1.0\n",
      "in iteration20 reward is: 1.0\n",
      "in iteration21 reward is: 1.0\n",
      "in iteration22 reward is: 1.0\n",
      "in iteration23 reward is: 1.0\n",
      "in iteration24 reward is: 1.0\n",
      "in iteration25 reward is: 1.0\n",
      "in iteration26 reward is: 1.0\n",
      "in iteration27 reward is: 1.0\n",
      "in iteration28 reward is: 1.0\n",
      "in iteration29 reward is: 1.0\n",
      "in iteration30 reward is: 1.0\n",
      "in iteration31 reward is: 1.0\n",
      "in iteration32 reward is: 1.0\n",
      "in iteration33 reward is: 1.0\n",
      "in iteration34 reward is: 1.0\n",
      "in iteration35 reward is: 1.0\n",
      "in iteration36 reward is: 1.0\n",
      "in iteration37 reward is: 1.0\n",
      "in iteration38 reward is: 1.0\n",
      "in iteration39 reward is: 1.0\n",
      "in iteration40 reward is: 1.0\n",
      "in iteration41 reward is: 1.0\n",
      "in iteration42 reward is: 1.0\n",
      "in iteration43 reward is: 1.0\n",
      "in iteration44 reward is: 1.0\n",
      "in iteration45 reward is: 1.0\n",
      "in iteration46 reward is: 1.0\n",
      "in iteration47 reward is: 1.0\n",
      "in iteration48 reward is: 1.0\n",
      "in iteration49 reward is: 1.0\n",
      "in iteration50 reward is: 1.0\n",
      "in iteration51 reward is: 1.0\n",
      "in iteration52 reward is: 1.0\n",
      "in iteration53 reward is: 1.0\n",
      "in iteration54 reward is: 1.0\n",
      "in iteration55 reward is: 1.0\n",
      "in iteration56 reward is: 1.0\n",
      "in iteration57 reward is: 1.0\n",
      "in iteration58 reward is: 1.0\n",
      "in iteration59 reward is: 1.0\n",
      "in iteration60 reward is: 1.0\n",
      "in iteration61 reward is: 1.0\n",
      "in iteration62 reward is: 1.0\n",
      "in iteration63 reward is: 1.0\n",
      "in iteration64 reward is: 1.0\n",
      "in iteration65 reward is: 1.0\n",
      "in iteration66 reward is: 1.0\n",
      "in iteration67 reward is: 1.0\n",
      "in iteration68 reward is: 1.0\n",
      "in iteration69 reward is: 1.0\n",
      "in iteration70 reward is: 1.0\n",
      "in iteration71 reward is: 1.0\n",
      "in iteration72 reward is: 1.0\n",
      "in iteration73 reward is: 1.0\n",
      "in iteration74 reward is: 1.0\n",
      "in iteration75 reward is: 1.0\n",
      "in iteration76 reward is: 1.0\n",
      "in iteration77 reward is: 1.0\n",
      "in iteration78 reward is: 1.0\n",
      "in iteration79 reward is: 1.0\n",
      "in iteration80 reward is: 1.0\n",
      "in iteration81 reward is: 1.0\n",
      "in iteration82 reward is: 1.0\n",
      "in iteration83 reward is: 1.0\n",
      "in iteration84 reward is: 1.0\n",
      "in iteration85 reward is: 1.0\n",
      "in iteration86 reward is: 1.0\n",
      "in iteration87 reward is: 1.0\n",
      "in iteration88 reward is: 1.0\n",
      "in iteration89 reward is: 1.0\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray import tune\n",
    "import ray\n",
    "import gym\n",
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper\n",
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper_multi_instances\n",
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper_no_action_mask\n",
    "from src.jss_lite.jss_lite import jss_lite\n",
    "from ray import tune\n",
    "curr_dir='/Users/felix/sciebo/masterarbeit/progra/model-based_rl'\n",
    "inst='/Users/felix/sciebo/masterarbeit/progra/model-based_rl/resources/jsp_instances/ima/3x3x3/3x3_4_inst.json'#curr_dir + '/resources/jsp_instances/standard/la01.txt'\n",
    "# Configure the algorithm.\n",
    "def env_creator(env_config):\n",
    "    env=jssp_light_obs_wrapper_no_action_mask(jssp_light_obs_wrapper_multi_instances(instances_list=[inst]))\n",
    "    return env\n",
    "from wrapper.jssp_wrapper_klagenfurt import jssp_klagenfurt_obs_wrapper\n",
    "import JSSEnv\n",
    "ray.shutdown()\n",
    "tune.register_env('custom_jssp',env_creator)\n",
    "config = {\n",
    "    \"env\": \"custom_jssp\",\n",
    "    \"disable_env_checking\":True,\n",
    "    \"num_workers\": 4,\n",
    "    \"framework\": \"tf\",\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [128, 128],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    \"evaluation_num_workers\": 0,\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": False,\n",
    "    },\n",
    "    \n",
    "}\n",
    "trainer = PPOTrainer(config=config)\n",
    "\n",
    "env=jssp_light_obs_wrapper_no_action_mask(jssp_light_obs_wrapper_multi_instances(instances_list=['/Users/felix/sciebo/masterarbeit/progra/model-based_rl/resources/jsp_instances/ima/3x3x3/3x3_4_inst.json']))\n",
    "for _ in range(1,90):\n",
    "    #print('/Users/felix/sciebo/masterarbeit/progra/model-based_rl/training_checkpoints/oneenvironment_punish_v2ima_8_8_ppo/checkpoint-'+str(_))\n",
    "    trainer.load_checkpoint('/Users/felix/sciebo/masterarbeit/progra/model-based_rl/training_checkpoints/oneenvironment_no_punish_v2ima_3_3_ppo/checkpoint-'+str(_))\n",
    "    import numpy as np\n",
    "    #trainer.save_checkpoint(\"ppo\")\n",
    "    episode_reward = 0\n",
    "    #env=jssp_light_obs_wrapper_multi_instances(instances_list=[inst])\n",
    "    #env=jssp_light_obs_wrapper_no_action_mask(jssp_light_obs_wrapper_multi_instances(instances_list=[inst]))\n",
    "    #env=env_creator(\"s\")\n",
    "    #env=jssp_light_obs_wrapper_no_action_mask(jssp_klagenfurt_obs_wrapper(gym.make('jss-v1', env_config={'instance_path': 'resources/jsp_instances/standard/la01.txt'})))\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    iterations=0\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        policy = trainer.get_policy()\n",
    "        action, _lk, info = policy.compute_single_action(obs)\n",
    "        logits = info['action_dist_inputs']\n",
    "        action_mask = env.get_legal_actions().astype(np.float32)\n",
    "        action_mask[action_mask==0] = - np.inf\n",
    "        probs=np.multiply(logits,action_mask)\n",
    "        probs[probs==np.inf]=-np.inf\n",
    "        best_action_id = probs.argmax()  # deterministic\n",
    "        #best_action_id = logits.argmax()\n",
    "        #best_action_id = trainer.compute_single_action(obs)\n",
    "        obs, reward, done, info = env.step(best_action_id)\n",
    "        episode_reward += reward\n",
    "        iterations += 1\n",
    "\n",
    "        #print(best_action_id)\n",
    "    #env.env.env.render()\n",
    "    #print(iterations)\n",
    "    #print(reward)\n",
    "    print(f\"in iteration{_} reward is: {reward}\")\n",
    "    #print(env.env.env.last_time_step-666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in iteration1 reward is: 0.5217391304347826\n",
      "in iteration2 reward is: 0.6086956521739131\n",
      "in iteration3 reward is: 0.5652173913043479\n",
      "in iteration4 reward is: 0.6231884057971014\n",
      "in iteration5 reward is: 0.536231884057971\n",
      "in iteration6 reward is: 0.6376811594202898\n",
      "in iteration7 reward is: 0.7246376811594203\n",
      "in iteration8 reward is: 0.6086956521739131\n",
      "in iteration9 reward is: 0.7101449275362319\n",
      "in iteration10 reward is: 0.7391304347826086\n",
      "in iteration11 reward is: 0.5797101449275363\n",
      "in iteration12 reward is: 0.6086956521739131\n",
      "in iteration13 reward is: 0.5217391304347826\n",
      "in iteration14 reward is: 0.536231884057971\n",
      "in iteration15 reward is: 0.7971014492753623\n",
      "in iteration16 reward is: 0.6086956521739131\n",
      "in iteration17 reward is: 0.6666666666666667\n",
      "in iteration18 reward is: 0.5942028985507246\n",
      "in iteration19 reward is: 0.4492753623188406\n",
      "in iteration20 reward is: 0.6521739130434783\n",
      "in iteration21 reward is: 0.5942028985507246\n",
      "in iteration22 reward is: 0.6086956521739131\n",
      "in iteration23 reward is: 0.7536231884057971\n",
      "in iteration24 reward is: 0.6811594202898551\n",
      "in iteration25 reward is: 0.7391304347826086\n",
      "in iteration26 reward is: 0.8115942028985508\n",
      "in iteration27 reward is: 0.7536231884057971\n",
      "in iteration28 reward is: 0.7391304347826086\n",
      "in iteration29 reward is: 0.7391304347826086\n",
      "in iteration30 reward is: 0.6376811594202898\n",
      "in iteration31 reward is: 0.6376811594202898\n",
      "in iteration32 reward is: 0.6376811594202898\n",
      "in iteration33 reward is: 0.6376811594202898\n",
      "in iteration34 reward is: 0.7391304347826086\n",
      "in iteration35 reward is: 0.7536231884057971\n",
      "in iteration36 reward is: 0.7681159420289855\n",
      "in iteration37 reward is: 0.7681159420289855\n",
      "in iteration38 reward is: 0.7681159420289855\n",
      "in iteration39 reward is: 0.7681159420289855\n",
      "in iteration40 reward is: 0.7681159420289855\n",
      "in iteration41 reward is: 0.7971014492753623\n",
      "in iteration42 reward is: 0.7971014492753623\n",
      "in iteration43 reward is: 0.7971014492753623\n",
      "in iteration44 reward is: 0.8985507246376812\n",
      "in iteration45 reward is: 0.8985507246376812\n",
      "in iteration46 reward is: 0.8985507246376812\n",
      "in iteration47 reward is: 0.8405797101449275\n",
      "in iteration48 reward is: 0.8695652173913043\n",
      "in iteration49 reward is: 0.8260869565217391\n",
      "in iteration50 reward is: 0.7971014492753623\n",
      "in iteration51 reward is: 0.7971014492753623\n",
      "in iteration52 reward is: 0.8695652173913043\n",
      "in iteration53 reward is: 0.8695652173913043\n",
      "in iteration54 reward is: 0.8260869565217391\n",
      "in iteration55 reward is: 0.9130434782608696\n",
      "in iteration56 reward is: 0.9130434782608696\n",
      "in iteration57 reward is: 0.8695652173913043\n",
      "in iteration58 reward is: 0.8260869565217391\n",
      "in iteration59 reward is: 0.8260869565217391\n",
      "in iteration60 reward is: 0.8405797101449275\n",
      "in iteration61 reward is: 0.8260869565217391\n",
      "in iteration62 reward is: 0.8260869565217391\n",
      "in iteration63 reward is: 0.8405797101449275\n",
      "in iteration64 reward is: 0.7391304347826086\n",
      "in iteration65 reward is: 0.7391304347826086\n",
      "in iteration66 reward is: 0.8260869565217391\n",
      "in iteration67 reward is: 0.7391304347826086\n",
      "in iteration68 reward is: 0.8260869565217391\n",
      "in iteration69 reward is: 0.6086956521739131\n",
      "in iteration70 reward is: 0.6086956521739131\n",
      "in iteration71 reward is: 0.6086956521739131\n",
      "in iteration72 reward is: 0.6086956521739131\n",
      "in iteration73 reward is: 0.6086956521739131\n",
      "in iteration74 reward is: 0.6811594202898551\n",
      "in iteration75 reward is: 0.782608695652174\n",
      "in iteration76 reward is: 0.6086956521739131\n",
      "in iteration77 reward is: 0.6086956521739131\n",
      "in iteration78 reward is: 0.6086956521739131\n",
      "in iteration79 reward is: 0.6086956521739131\n",
      "in iteration80 reward is: 0.8405797101449275\n",
      "in iteration81 reward is: 0.6086956521739131\n",
      "in iteration82 reward is: 0.6086956521739131\n",
      "in iteration83 reward is: 0.6086956521739131\n",
      "in iteration84 reward is: 0.5797101449275363\n",
      "in iteration85 reward is: 0.6086956521739131\n",
      "in iteration86 reward is: 0.6086956521739131\n",
      "in iteration87 reward is: 0.6086956521739131\n",
      "in iteration88 reward is: 0.6086956521739131\n",
      "in iteration89 reward is: 0.6086956521739131\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env=jssp_light_obs_wrapper_no_action_mask(jssp_light_obs_wrapper_multi_instances(instances_list=['/Users/felix/sciebo/masterarbeit/progra/model-based_rl/resources/jsp_instances/ima/8x8x8/8x8_5_inst.json']))\n",
    "for _ in range(1,90):\n",
    "    #print('/Users/felix/sciebo/masterarbeit/progra/model-based_rl/training_checkpoints/oneenvironment_punish_v2ima_8_8_ppo/checkpoint-'+str(_))\n",
    "    trainer.load_checkpoint('/Users/felix/sciebo/masterarbeit/progra/model-based_rl/training_checkpoints/oneenvironment_punish_v2ima_8_8_ppo/checkpoint-'+str(_))\n",
    "    import numpy as np\n",
    "    #trainer.save_checkpoint(\"ppo\")\n",
    "    episode_reward = 0\n",
    "    #env=jssp_light_obs_wrapper_multi_instances(instances_list=[inst])\n",
    "    #env=jssp_light_obs_wrapper_no_action_mask(jssp_light_obs_wrapper_multi_instances(instances_list=[inst]))\n",
    "    #env=env_creator(\"s\")\n",
    "    #env=jssp_light_obs_wrapper_no_action_mask(jssp_klagenfurt_obs_wrapper(gym.make('jss-v1', env_config={'instance_path': 'resources/jsp_instances/standard/la01.txt'})))\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    iterations=0\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        policy = trainer.get_policy()\n",
    "        action, _lk, info = policy.compute_single_action(obs)\n",
    "        logits = info['action_dist_inputs']\n",
    "        action_mask = env.get_legal_actions().astype(np.float32)\n",
    "        action_mask[action_mask==0] = - np.inf\n",
    "        probs=np.multiply(logits,action_mask)\n",
    "        probs[probs==np.inf]=-np.inf\n",
    "        best_action_id = probs.argmax()  # deterministic\n",
    "        #best_action_id = logits.argmax()\n",
    "        #best_action_id = trainer.compute_single_action(obs)\n",
    "        obs, reward, done, info = env.step(best_action_id)\n",
    "        episode_reward += reward\n",
    "        iterations += 1\n",
    "\n",
    "        #print(best_action_id)\n",
    "    #env.env.env.render()\n",
    "    #print(iterations)\n",
    "    #print(reward)\n",
    "    print(f\"in iteration{_} reward is: {reward}\")\n",
    "    #print(env.env.env.last_time_step-666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931297709923664\n"
     ]
    }
   ],
   "source": [
    "\n",
    "needed_time=env.env.env.last_time_step\n",
    "optimal_value=655\n",
    "reward=1-((needed_time-optimal_value)/optimal_value)\n",
    "print(reward)\n",
    "#print(datetime.datetime.fromtimestamp(env.env.env.start_timestamp+env.env.env.last_time_step))\n",
    "#print((env.env.env.start_timestamp))\n",
    "#print(datetime.datetime.fromtimestamp(env.env.env.start_timestamp))\n",
    "\n",
    "#print(env.env.env.last_time_step)\n",
    "#print(datetime.datetime.fromtimestamp(env.env.env.last_time_step))\n",
    "#print(10*60*60-9*60*60+6*60-54*60+56-2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.env.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_2=jssp_light_obs_wrapper_multi_instances(instances_list=[inst])\n",
    "x,=env_2.observation_space['obs'].shape\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iterations)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info)\n",
    "print(len(info['action_dist_inputs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper\n",
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper_multi_instances\n",
    "from src.jss_lite.jss_lite import jss_lite\n",
    "curr_dir='/Users/felix/sciebo/masterarbeit/progra/model-based_rl'\n",
    "inst=curr_dir + '/resources/jsp_instances/standard/ft06.txt'\n",
    "instance_list=['/resources/jsp_instances/standard/la01.txt']#,'/resources/jsp_instances/standard/la02.txt','/resources/jsp_instances/standard/la03.txt','/resources/jsp_instances/standard/la04.txt']\n",
    "# Configure the algorithm.\n",
    "def env_creator(env_config):\n",
    "    env=jssp_light_obs_wrapper_multi_instances(instances_list=[inst])\n",
    "    #env=jss_lite(instance_path=inst)\n",
    "    return env\n",
    "# get random actions:\n",
    "reward=0\n",
    "while reward <0.95:\n",
    "#for _ in range(100):\n",
    "    action_list=[]\n",
    "    env = env_creator(\"s\")\n",
    "\n",
    "    obs = env.reset()\n",
    "    # env2 is copy for later going evaluation\n",
    "    counter=0\n",
    "    done = False\n",
    "    t=time.time()\n",
    "    while not done:\n",
    "        legal_action=obs['action_mask']\n",
    "        action=np.random.choice(len(legal_action), 1, p=(legal_action / legal_action.sum()))[0]\n",
    "        action_list.append(action)\n",
    "        #print(action)\n",
    "        #print(obs['action_mask'][action])\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        counter+=1\n",
    "    #print(f\"instance: {env.env.instance} in time: {time.time()-t}s\")\n",
    "    \n",
    "    #env.render()\n",
    "    #env.close()\n",
    "print(counter)\n",
    "print(reward)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x=np.array([2,2,2,2,2,2,2,2,2,2,2,2,4])\n",
    "print(x[x>=0.8*2].argmax())\n",
    "y,=(np.where(x>=2))\n",
    "\n",
    "for i in y:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('customjssp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Mar 31 2022, 03:38:35) [Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7344f7b5995cbf62a990a56ee6eec8bd53f41a3aff848cd18f1feb05905aa9e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
