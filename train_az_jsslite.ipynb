{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-13 12:25:47,183\tWARNING deprecation.py:46 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-10-13 12:26:05,677\tINFO trainable.py:159 -- Trainable.setup took 18.495 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    }
   ],
   "source": [
    "from ray import rllib, tune\n",
    "import ray\n",
    "from ray.rllib.contrib.alpha_zero.core.alpha_zero_trainer import AlphaZeroTrainer\n",
    "from ray.rllib.policy.sample_batch import DEFAULT_POLICY_ID\n",
    "from ray.rllib.policy.policy_map import PolicyMap\n",
    "from ray.rllib.evaluation.episode import MultiAgentEpisode\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.contrib.alpha_zero.models.custom_torch_models import DenseModel\n",
    "from ray.rllib.models.catalog import ModelCatalog\n",
    "import gym\n",
    "from src.jss_lite.jss_lite import jss_lite\n",
    "ModelCatalog.register_custom_model(\"dense_model\", DenseModel)\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import os\n",
    "ray.shutdown()\n",
    "#curr_dir=(os.path.dirname(__file__))\n",
    "curr_dir='/Users/felix/sciebo/masterarbeit/progra/model-based_rl'\n",
    "instance_list=['/resources/jsp_instances/standard/la01.txt','/resources/jsp_instances/standard/la02.txt','/resources/jsp_instances/standard/la03.txt','/resources/jsp_instances/standard/la04.txt','/resources/jsp_instances/standard/la05.txt']\n",
    "instance_list=[curr_dir + s for s in instance_list]\n",
    "#instance_list=['resources/jsp_instances/standard/ft06.txt','resources/jsp_instances/standard/ft10.txt','resources/jsp_instances/standard/ft20.txt','resources/jsp_instances/standard/abz5.txt','resources/jsp_instances/standard/abz6.txt','resources/jsp_instances/standard/abz7.txt']\n",
    "#instance_list=['resources/jsp_instances/standard/ft06.txt']\n",
    "\n",
    "train_agent=True\n",
    "instance_path='resources/jsp_instances/standard/ft06.txt'\n",
    "restore_agent= False\n",
    "num_episodes = 5\n",
    "restore_path= '/Users/felix/sciebo/masterarbeit/progra/model-based_rl/published_checkpoints/checkpoints_az_tune/ft06/checkpoint_000100/checkpoint-100'\n",
    "config = {\n",
    "    \"framework\": \"torch\",\n",
    "    \"disable_env_checking\":True,\n",
    "    \"num_workers\"       : 6,\n",
    "    \"rollout_fragment_length\": 50,\n",
    "    \"train_batch_size\"  : 500,\n",
    "    \"sgd_minibatch_size\": 64,\n",
    "    \"lr\"                : 0.0001,\n",
    "    \"explore\"           :True,\n",
    "    #\"horizon\"           : 600,\n",
    "    #\"soft_horizon\"      : True,\n",
    "    \"num_sgd_iter\"      : 1,\n",
    "    #\"horizon\"           : 100,\n",
    "    \"mcts_config\"       : {\n",
    "        \"puct_coefficient\"   : 1.5,\n",
    "        \"num_simulations\"    : 100,\n",
    "        \"temperature\"        : 1.0,\n",
    "        \"dirichlet_epsilon\"  : 0.20,\n",
    "        \"dirichlet_noise\"    : 0.03,\n",
    "        \"argmax_tree_policy\" : False,\n",
    "        \"add_dirichlet_noise\": False,\n",
    "    },\n",
    "    \"ranked_rewards\"    : {\n",
    "        \"enable\": True,\n",
    "    },\n",
    "    \"model\"             : {\n",
    "        \"custom_model\": \"dense_model\",\n",
    "\n",
    "    },\n",
    "}\n",
    "\n",
    "# def env_creator(env_config):\n",
    "#     env = jss_lite(instance_path='resources/jsp_instances/standard/ft06.txt')\n",
    "#     return env\n",
    "\n",
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper\n",
    "#from wrapper.jssplight_wrapper import jssp_light_obs_wrapper_multi_instances\n",
    "\n",
    "def env_creator(config):\n",
    "    env = jssp_light_obs_wrapper(jss_lite(instance_path=instance_path))\n",
    "    #env=jssp_light_obs_wrapper_multi_instances(instances_list=instance_list)\n",
    "    return env\n",
    "\n",
    "ModelCatalog.register_custom_model(\"dense_model\", DenseModel)    \n",
    "\n",
    "\n",
    "\n",
    "# use tune to register the custom environment for the ppo trainer\n",
    "tune.register_env('custom_jssp',env_creator)\n",
    "\n",
    "agent = AlphaZeroTrainer( config=config, env='custom_jssp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "training iteration 0 finished after 48.28817820549011 seconds\n",
      "training iteration 1 finished after 51.97125482559204 seconds\n",
      "training iteration 2 finished after 41.261812925338745 seconds\n",
      "training iteration 3 finished after 42.23966717720032 seconds\n",
      "training iteration 4 finished after 42.53530502319336 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "if train_agent:\n",
    "    # checkpoint_path = analysis.get_last_checkpoint() or args.checkpoint\n",
    "    ## use string number to restore pre trained agent\n",
    "    # nr_restore=\"10\"\n",
    "    #checkpoint_path=f'checkpoints_az/rllib_checkpoint{nr_restore}/checkpoint_{nr_restore.zfill(6)}/checkpoint-{nr_restore}'\n",
    "    #agent.load_checkpoint(\"training_checkpoints/checkpoints_az_jsslite/checkpoint-5\")\n",
    "    #print(\"awd\")\n",
    "    #agent.restore(\"checkpoints_az/rllib_checkpoint1/checkpoint_000001/checkpoint-1\")\n",
    "    #agent.load_checkpoint(\"published_checkpoints/az_taxi/checkpoint-34\")\n",
    "    print(\"start training\")\n",
    "    for _ in range(0,num_episodes):\n",
    "        t=time.time()\n",
    "        agent.train()\n",
    "        print(f\"training iteration {_} finished after {time.time()-t} seconds\")\n",
    "        agent.save_checkpoint(f\"training_checkpoints/checkpoints_az_jsslite_ft06\")\n",
    "    \n",
    "if restore_agent:\n",
    "\n",
    "    agent.load_checkpoint(restore_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-13 12:33:49,242\tWARNING deprecation.py:46 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-10-13 12:33:59,561\tINFO trainable.py:159 -- Trainable.setup took 10.321 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[36m(scheduler +8m17s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +8m17s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "instance: ft06 in time: 6.6454620361328125s\n",
      "32.727272727272734\n",
      "instance: ft06 in time: 9.228685855865479s\n",
      "32.727272727272734\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/felix/sciebo/masterarbeit/progra/model-based_rl/train_az_jsslite.ipynb Zelle 3\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felix/sciebo/masterarbeit/progra/model-based_rl/train_az_jsslite.ipynb#W2sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m t\u001b[39m=\u001b[39mtime\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felix/sciebo/masterarbeit/progra/model-based_rl/train_az_jsslite.ipynb#W2sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/felix/sciebo/masterarbeit/progra/model-based_rl/train_az_jsslite.ipynb#W2sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     action, _, _ \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39;49mcompute_single_action(obs, episode\u001b[39m=\u001b[39;49mepisode)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felix/sciebo/masterarbeit/progra/model-based_rl/train_az_jsslite.ipynb#W2sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     action_list\u001b[39m.\u001b[39mappend(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felix/sciebo/masterarbeit/progra/model-based_rl/train_az_jsslite.ipynb#W2sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     \u001b[39m#print(action_dic[action])\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/policy/policy.py:255\u001b[0m, in \u001b[0;36mPolicy.compute_single_action\u001b[0;34m(self, obs, state, prev_action, prev_reward, info, input_dict, episode, explore, timestep, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39mif\u001b[39;00m episode \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     episodes \u001b[39m=\u001b[39m [episode]\n\u001b[0;32m--> 255\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_actions_from_input_dict(\n\u001b[1;32m    256\u001b[0m     input_dict\u001b[39m=\u001b[39;49mSampleBatch(input_dict),\n\u001b[1;32m    257\u001b[0m     episodes\u001b[39m=\u001b[39;49mepisodes,\n\u001b[1;32m    258\u001b[0m     explore\u001b[39m=\u001b[39;49mexplore,\n\u001b[1;32m    259\u001b[0m     timestep\u001b[39m=\u001b[39;49mtimestep,\n\u001b[1;32m    260\u001b[0m )\n\u001b[1;32m    262\u001b[0m \u001b[39m# Some policies don't return a tuple, but always just a single action.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[39m# E.g. ES and ARS.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/contrib/alpha_zero/core/alpha_zero_policy.py:97\u001b[0m, in \u001b[0;36mAlphaZeroPolicy.compute_actions_from_input_dict\u001b[0;34m(self, input_dict, explore, timestep, episodes, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     tree_node \u001b[39m=\u001b[39m episode\u001b[39m.\u001b[39muser_data[\u001b[39m\"\u001b[39m\u001b[39mtree_node\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     95\u001b[0m \u001b[39m# run monte carlo simulations to compute the actions\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39m# and record the tree\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m mcts_policy, action, tree_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmcts\u001b[39m.\u001b[39;49mcompute_action(tree_node)\n\u001b[1;32m     98\u001b[0m \u001b[39m# record action\u001b[39;00m\n\u001b[1;32m     99\u001b[0m actions\u001b[39m.\u001b[39mappend(action)\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/contrib/alpha_zero/core/mcts.py:129\u001b[0m, in \u001b[0;36mMCTS.compute_action\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_action\u001b[39m(\u001b[39mself\u001b[39m, node):\n\u001b[1;32m    128\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_sims):\n\u001b[0;32m--> 129\u001b[0m         leaf \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39;49mselect()\n\u001b[1;32m    130\u001b[0m         \u001b[39mif\u001b[39;00m leaf\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    131\u001b[0m             value \u001b[39m=\u001b[39m leaf\u001b[39m.\u001b[39mreward\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/contrib/alpha_zero/core/mcts.py:77\u001b[0m, in \u001b[0;36mNode.select\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mwhile\u001b[39;00m current_node\u001b[39m.\u001b[39mis_expanded:\n\u001b[1;32m     76\u001b[0m     best_action \u001b[39m=\u001b[39m current_node\u001b[39m.\u001b[39mbest_action()\n\u001b[0;32m---> 77\u001b[0m     current_node \u001b[39m=\u001b[39m current_node\u001b[39m.\u001b[39;49mget_child(best_action)\n\u001b[1;32m     78\u001b[0m \u001b[39mreturn\u001b[39;00m current_node\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/contrib/alpha_zero/core/mcts.py:88\u001b[0m, in \u001b[0;36mNode.get_child\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mset_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate)\n\u001b[1;32m     87\u001b[0m     obs, reward, done, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m---> 88\u001b[0m     next_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mget_state()\n\u001b[1;32m     89\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren[action] \u001b[39m=\u001b[39m Node(\n\u001b[1;32m     90\u001b[0m         state\u001b[39m=\u001b[39mnext_state,\n\u001b[1;32m     91\u001b[0m         action\u001b[39m=\u001b[39maction,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m         mcts\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmcts,\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     98\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren[action]\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/contrib/alpha_zero/core/ranked_rewards.py:68\u001b[0m, in \u001b[0;36mget_r2_env_wrapper.<locals>.RankedRewardsEnvWrapper.get_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     64\u001b[0m     state \u001b[39m=\u001b[39m {\n\u001b[1;32m     65\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39menv_state\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mget_state(),\n\u001b[1;32m     66\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbuffer_state\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mr2_buffer\u001b[39m.\u001b[39mget_state(),\n\u001b[1;32m     67\u001b[0m     }\n\u001b[0;32m---> 68\u001b[0m     \u001b[39mreturn\u001b[39;00m deepcopy(state)\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39;49mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[39m=\u001b[39m deepcopy(state, memo)\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(y, \u001b[39m'\u001b[39m\u001b[39m__setstate__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[39m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/copy.py:177\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n\u001b[1;32m    176\u001b[0m     memo[d] \u001b[39m=\u001b[39m y\n\u001b[0;32m--> 177\u001b[0m     _keep_alive(x, memo) \u001b[39m# Make sure x lives at least as long as d\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/copy.py:254\u001b[0m, in \u001b[0;36m_keep_alive\u001b[0;34m(x, memo)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39m\"\"\"Keeps a reference to the object x in the memo.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \n\u001b[1;32m    246\u001b[0m \u001b[39mBecause we remember objects by their id, we have\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mthe memo itself...\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m     memo[\u001b[39mid\u001b[39;49m(memo)]\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m    255\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m     \u001b[39m# aha, this is the first one :-)\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     memo[\u001b[39mid\u001b[39m(memo)]\u001b[39m=\u001b[39m[x]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "config = {\n",
    "    \"framework\": \"torch\",\n",
    "    \"disable_env_checking\":True,\n",
    "    \"num_workers\"       : 6,\n",
    "    \"rollout_fragment_length\": 50,\n",
    "    \"train_batch_size\"  : 500,\n",
    "    \"sgd_minibatch_size\": 64,\n",
    "    \"lr\"                : 0.0001,\n",
    "    #\"explore\"           :False,\n",
    "    #\"horizon\"           : 600,\n",
    "    #\"soft_horizon\"      : True,\n",
    "    \"num_sgd_iter\"      : 1,\n",
    "    #\"horizon\"           : 100,\n",
    "    \"mcts_config\"       : {\n",
    "        \"puct_coefficient\"   : 1.5,\n",
    "        \"num_simulations\"    : 2000,\n",
    "        \"temperature\"        : 1.5,\n",
    "        \"dirichlet_epsilon\"  : 0.20,\n",
    "        \"dirichlet_noise\"    : 0.03,\n",
    "        \"argmax_tree_policy\" : True,\n",
    "        \"add_dirichlet_noise\": False,\n",
    "    },\n",
    "    \"ranked_rewards\"    : {\n",
    "        \"enable\": True,\n",
    "    },\n",
    "    \"model\"             : {\n",
    "        \"custom_model\": \"dense_model\",\n",
    "\n",
    "    },\n",
    "}\n",
    "\n",
    "agent = AlphaZeroTrainer( config=config, env='custom_jssp')\n",
    "#if restore_agent:\n",
    "#restore_path='training_checkpoints/checkpoints_az_jsslite/checkpoint-5'\n",
    "restore_path='/Users/felix/sciebo/masterarbeit/progra/model-based_rl/published_checkpoints/checkpoints_az_tune/ft06/checkpoint_000100/checkpoint-100'\n",
    "agent.load_checkpoint(restore_path)\n",
    "\n",
    "#agent.load_checkpoint(\"training_checkpoints/checkpoints_az_jsslite/checkpoint-5\")\n",
    "for _ in range(10):\n",
    "    import time\n",
    "    length_list=[]\n",
    "    reward_list=[]\n",
    "    for _ in range(1):\n",
    "        policy = agent.get_policy(DEFAULT_POLICY_ID)\n",
    "        action_list=[]\n",
    "        env = env_creator(\"s\")\n",
    "\n",
    "        obs = env.reset()\n",
    "        # env2 is copy for later going evaluation\n",
    "    \n",
    "\n",
    "        episode = MultiAgentEpisode(\n",
    "            PolicyMap(0,0),\n",
    "            lambda _, __: DEFAULT_POLICY_ID,\n",
    "            lambda: None,\n",
    "            lambda _: None,\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        episode.user_data['initial_state'] = env.get_state()\n",
    "\n",
    "        done = False\n",
    "        t=time.time()\n",
    "        while not done:\n",
    "            action, _, _ = policy.compute_single_action(obs, episode=episode)\n",
    "            action_list.append(action)\n",
    "            #print(action_dic[action])\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            #print(obs)\n",
    "            #env.render(render_mode='human')\n",
    "            #time.sleep(0.1)\n",
    "            episode.length += 1\n",
    "\n",
    "        length_list.append(episode.length)\n",
    "        reward_list.append(reward)\n",
    "        print(f\"instance: {env.env.instance} in time: {time.time()-t}s\")\n",
    "        print(reward)\n",
    "        #env.render()\n",
    "        #env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "\n",
    "tune.run(\n",
    "    \"contrib/AlphaZero\",\n",
    "    stop={\"training_iteration\": 10},\n",
    "    max_failures=0,\n",
    "    #restore='/Users/felix/sciebo/masterarbeit/progra/model-based_rl/published_checkpoints/checkpoints_az_tune/checkpoint_000500/checkpoint-500',\n",
    "    config={\n",
    "        \"env\": 'custom_jssp',\n",
    "        \"num_workers\": 4,\n",
    "        \"rollout_fragment_length\": 50,\n",
    "        \"train_batch_size\": 50,\n",
    "        \"sgd_minibatch_size\": 32,\n",
    "        \"lr\": 1e-4,\n",
    "        \"num_sgd_iter\": 1,\n",
    "        \"mcts_config\": {\n",
    "            \"puct_coefficient\": 1.5,\n",
    "            \"num_simulations\": 100,\n",
    "            \"temperature\": 1.0,\n",
    "            \"dirichlet_epsilon\": 0.20,\n",
    "            \"dirichlet_noise\": 0.03,\n",
    "            \"argmax_tree_policy\": False,\n",
    "            \"add_dirichlet_noise\": False,\n",
    "        },\n",
    "        \"ranked_rewards\": {\n",
    "            \"enable\": True,\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"custom_model\": \"dense_model\",\n",
    "        },\n",
    "        \"evaluation_interval\": 1,\n",
    "        \"evaluation_config\": {\n",
    "            \"render_env\": True,\n",
    "            \"mcts_config\": {\n",
    "                \"argmax_tree_policy\": True,\n",
    "                \"add_dirichlet_noise\": False,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = env_creator(\"setting\")\n",
    "# config.update(\n",
    "#     simple_optimizer=True,\n",
    "#     num_workers=0,\n",
    "#     train_batch_size=0,\n",
    "#     rollout_fragment_length=0,\n",
    "#     timesteps_per_iteration=0,\n",
    "#     evaluation_interval=1,\n",
    "#     # evaluation_num_workers=...,\n",
    "#     # evaluation_config=dict(explore=False),\n",
    "#     # evaluation_num_episodes=...,\n",
    "# )\n",
    "\n",
    "# results = tune.run(\n",
    "#     agent,\n",
    "#     config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.load_checkpoint(\"training_checkpoints/checkpoints_az_jsslite/checkpoint-5\")\n",
    "for _ in range(10):\n",
    "    import time\n",
    "    length_list=[]\n",
    "    reward_list=[]\n",
    "    for _ in range(1):\n",
    "        policy = agent.get_policy(DEFAULT_POLICY_ID)\n",
    "        action_list=[]\n",
    "        env = env_creator(\"s\")\n",
    "\n",
    "        obs = env.reset()\n",
    "        # env2 is copy for later going evaluation\n",
    "        env2=deepcopy(env)\n",
    "\n",
    "        episode = MultiAgentEpisode(\n",
    "            PolicyMap(0,0),\n",
    "            lambda _, __: DEFAULT_POLICY_ID,\n",
    "            lambda: None,\n",
    "            lambda _: None,\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        episode.user_data['initial_state'] = env.get_state()\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, _, _ = policy.compute_single_action(obs, episode=episode)\n",
    "            action_list.append(action)\n",
    "            #print(action_dic[action])\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            #print(obs)\n",
    "            #env.render(render_mode='human')\n",
    "            #time.sleep(0.1)\n",
    "            episode.length += 1\n",
    "\n",
    "        length_list.append(episode.length)\n",
    "        reward_list.append(reward)\n",
    "        print(reward)\n",
    "        env.render()\n",
    "        #env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.env.current_timestep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.env.done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.env.production_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_1=env.reset()\n",
    "#print(obs_1)\n",
    "\n",
    "env_1=env_creator(\"a\")\n",
    "env_2=env_creator(\"b\")\n",
    "state_1=env_1.reset()\n",
    "state_2=env_2.reset()\n",
    "\n",
    "for a in a_list:\n",
    "    state_1, reward_1, done_1, info_1 = env_1.step(a)\n",
    "    state_2, reward_2, done_2, info_2 = env_2.step(a)\n",
    "\n",
    "    if np.array_equal(state_1['obs'],state_2['obs'])==False :\n",
    "        print(\"error obs\")\n",
    "        print(state_1)\n",
    "        print(state_2)\n",
    "    if np.array_equal(state_1['action_mask'],state_2['action_mask'])==False :\n",
    "        print(\"error mask\")\n",
    "        print(state_1)\n",
    "        print(state_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    \n",
    "    agent.load_checkpoint('training_checkpoints/checkpoints_az_jsslite/checkpoint-'+str(i+1))\n",
    "    policy = agent.get_policy(DEFAULT_POLICY_ID)\n",
    "    action_list=[]\n",
    "    env = env_creator(\"s\")\n",
    "\n",
    "    obs = env.reset()\n",
    "    # env2 is copy for later going evaluation\n",
    "    #env2=deepcopy(env)\n",
    "\n",
    "    episode = MultiAgentEpisode(\n",
    "        PolicyMap(0,0),\n",
    "        lambda _, __: DEFAULT_POLICY_ID,\n",
    "        lambda: None,\n",
    "        lambda _: None,\n",
    "        0,\n",
    "    )\n",
    "\n",
    "    episode.user_data['initial_state'] = env.get_state()\n",
    "\n",
    "    done = False\n",
    "    steps=0\n",
    "    t=time.time()\n",
    "    while not done:\n",
    "        action, _, _ = policy.compute_single_action(obs, episode=episode)\n",
    "        action_list.append(action)\n",
    "        #print(action_dic[action])\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        #print(obs)\n",
    "        #env.render(render_mode='human')\n",
    "        #time.sleep(0.1)\n",
    "        steps+=1\n",
    "    print(f\"checkpoint {i} got reward {reward} in {steps} steps and time: {time.time()-t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.env.job_tasklength_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reward)\n",
    "print(env.env.count_finished_tasks_job_matrix)\n",
    "env.render()\n",
    "env.render(y_bar=\"Machine\",x_bar=\"Job\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()\n",
    "#env.render(y_bar=\"Machine\",x_bar=\"Job\")\n",
    "print(done)\n",
    "print(reward)\n",
    "print(env.invalid_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "from random import randrange\n",
    "a_list=[]\n",
    "for _ in range(30):\n",
    "    state=env.reset()\n",
    "    done=False\n",
    "    i=0\n",
    "    reward=0\n",
    "    #print(state['obs'].shape)\n",
    "    #for i in range(150):\n",
    "    while not done:\n",
    "        i+=1\n",
    "        legal_action=state['action_mask']\n",
    "        #print(legal_action)\n",
    "        action=np.random.choice(len(legal_action), 1, p=(legal_action / legal_action.sum()))[0]\n",
    "        a_list.append(action)\n",
    "        #action=randrange(env.action_space.n/2)\n",
    "        #print(action)\n",
    "        token=True\n",
    "        #print(\"before:\")\n",
    "        #env.render(start_count=1,x_bar=\"Job\",y_bar=\"Machine\")\n",
    "        state, reward, done, info=env.step(action)\n",
    "    #env.render()\n",
    "    print(reward)\n",
    "    \n",
    "    if reward > -68:\n",
    "        print(env.env.get_legal_actions(\"yy\"))\n",
    "        print(env.done)\n",
    "        print(env.env.done)\n",
    "        print(env.env.production_list)\n",
    "        env.env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.env.get_legal_actions(\"a\"))\n",
    "print(env.env.current_timestep)\n",
    "print(env.env.current_machines_status)\n",
    "print(env.env.processed_and_max_time_job_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.env.get_legal_actions(\"a\"))\n",
    "print(env.env.blocked_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.env.blocked_actions)\n",
    "print(env.env.get_legal_actions(\"obs\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[[1,2,None],[1,2,3]]\n",
    "if any(x==None for x in x):\n",
    "        print(True)\n",
    "for row in x:\n",
    "    if None in row:\n",
    "        #print(True)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.env.current_machines_status)\n",
    "print(env.env.get_legal_actions(\"stat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('customjssp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7344f7b5995cbf62a990a56ee6eec8bd53f41a3aff848cd18f1feb05905aa9e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
