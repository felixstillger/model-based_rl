{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-24 17:51:17,978\tWARNING deprecation.py:47 -- DeprecationWarning: `config['learning_starts']` has been deprecated. config['replay_buffer_config']['learning_starts'] This will raise an error in the future!\n",
      "2022-10-24 17:51:17,984\tWARNING deprecation.py:47 -- DeprecationWarning: `config['multiagent']['replay_mode']` has been deprecated. config['replay_buffer_config']['replay_mode'] This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': <class 'ray.rllib.examples.env.cartpole_sparse_rewards.CartPoleSparseRewards'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 2, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'complete_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'custom_model': <class 'ray.rllib.algorithms.alpha_zero.models.custom_torch_models.DenseModel'>}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'mcts_config': {'argmax_tree_policy': True, 'add_dirichlet_noise': False}}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': 1000, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'sgd_minibatch_size': 256, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'replay_buffer_config': {'type': 'ReplayBuffer', 'capacity': 1000, 'learning_starts': 500, 'storage_unit': 'fragments'}, 'lr_schedule': None, 'vf_share_layers': False, 'mcts_config': {'puct_coefficient': 1.0, 'num_simulations': 30, 'temperature': 1.5, 'dirichlet_epsilon': 0.25, 'dirichlet_noise': 0.03, 'argmax_tree_policy': False, 'add_dirichlet_noise': True}, 'ranked_rewards': {'enable': True, 'percentile': 75, 'buffer_max_length': 1000, 'initialize_buffer': True, 'num_init_rewards': 100}, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.alpha_zero.alpha_zero.AlphaZeroDefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-24 17:51:20,963\tINFO worker.py:1518 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25205)\u001b[0m 2022-10-24 17:51:27,418\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-10-24 17:51:27,656\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-24 17:51:40,842\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25205)\u001b[0m 2022-10-24 17:51:40,833\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'custom_metrics': {},\n",
       " 'episode_media': {},\n",
       " 'num_recreated_workers': 0,\n",
       " 'info': {'learner': {'default_policy': {'learner_stats': {'total_loss': 0.3740916,\n",
       "     'policy_loss': 0.64926463,\n",
       "     'value_loss': 0.09891848}}},\n",
       "  'num_env_steps_sampled': 446,\n",
       "  'num_env_steps_trained': 891708,\n",
       "  'num_agent_steps_sampled': 446,\n",
       "  'num_agent_steps_trained': 891708},\n",
       " 'sampler_results': {'episode_reward_max': 37.0,\n",
       "  'episode_reward_min': 12.0,\n",
       "  'episode_reward_mean': 27.875,\n",
       "  'episode_len_mean': 27.875,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 16,\n",
       "  'policy_reward_min': {},\n",
       "  'policy_reward_max': {},\n",
       "  'policy_reward_mean': {},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [30.0,\n",
       "    31.0,\n",
       "    30.0,\n",
       "    32.0,\n",
       "    31.0,\n",
       "    30.0,\n",
       "    37.0,\n",
       "    26.0,\n",
       "    25.0,\n",
       "    26.0,\n",
       "    12.0,\n",
       "    26.0,\n",
       "    26.0,\n",
       "    27.0,\n",
       "    26.0,\n",
       "    31.0],\n",
       "   'episode_lengths': [30,\n",
       "    31,\n",
       "    30,\n",
       "    32,\n",
       "    31,\n",
       "    30,\n",
       "    37,\n",
       "    26,\n",
       "    25,\n",
       "    26,\n",
       "    12,\n",
       "    26,\n",
       "    26,\n",
       "    27,\n",
       "    26,\n",
       "    31]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.3134490234386329,\n",
       "   'mean_inference_ms': 54.734865183452186,\n",
       "   'mean_action_processing_ms': 0.07917461728950839,\n",
       "   'mean_env_wait_ms': 0.052426589820256296,\n",
       "   'mean_env_render_ms': 0.0},\n",
       "  'num_faulty_episodes': 0},\n",
       " 'episode_reward_max': 37.0,\n",
       " 'episode_reward_min': 12.0,\n",
       " 'episode_reward_mean': 27.875,\n",
       " 'episode_len_mean': 27.875,\n",
       " 'episodes_this_iter': 16,\n",
       " 'policy_reward_min': {},\n",
       " 'policy_reward_max': {},\n",
       " 'policy_reward_mean': {},\n",
       " 'hist_stats': {'episode_reward': [30.0,\n",
       "   31.0,\n",
       "   30.0,\n",
       "   32.0,\n",
       "   31.0,\n",
       "   30.0,\n",
       "   37.0,\n",
       "   26.0,\n",
       "   25.0,\n",
       "   26.0,\n",
       "   12.0,\n",
       "   26.0,\n",
       "   26.0,\n",
       "   27.0,\n",
       "   26.0,\n",
       "   31.0],\n",
       "  'episode_lengths': [30,\n",
       "   31,\n",
       "   30,\n",
       "   32,\n",
       "   31,\n",
       "   30,\n",
       "   37,\n",
       "   26,\n",
       "   25,\n",
       "   26,\n",
       "   12,\n",
       "   26,\n",
       "   26,\n",
       "   27,\n",
       "   26,\n",
       "   31]},\n",
       " 'sampler_perf': {'mean_raw_obs_processing_ms': 0.3134490234386329,\n",
       "  'mean_inference_ms': 54.734865183452186,\n",
       "  'mean_action_processing_ms': 0.07917461728950839,\n",
       "  'mean_env_wait_ms': 0.052426589820256296,\n",
       "  'mean_env_render_ms': 0.0},\n",
       " 'num_faulty_episodes': 0,\n",
       " 'num_healthy_workers': 2,\n",
       " 'num_agent_steps_sampled': 446,\n",
       " 'num_agent_steps_trained': 891708,\n",
       " 'num_env_steps_sampled': 446,\n",
       " 'num_env_steps_trained': 891708,\n",
       " 'num_env_steps_sampled_this_iter': 446,\n",
       " 'num_env_steps_trained_this_iter': 891708,\n",
       " 'timesteps_total': 446,\n",
       " 'num_steps_trained_this_iter': 891708,\n",
       " 'agent_timesteps_total': 446,\n",
       " 'timers': {'training_iteration_time_ms': 288065.377,\n",
       "  'load_time_ms': 0.295,\n",
       "  'load_throughput': 3021077892.756,\n",
       "  'learn_time_ms': 274408.47,\n",
       "  'learn_throughput': 3249.564,\n",
       "  'synch_weights_time_ms': 1.921},\n",
       " 'counters': {'num_env_steps_sampled': 446,\n",
       "  'num_env_steps_trained': 891708,\n",
       "  'num_agent_steps_sampled': 446,\n",
       "  'num_agent_steps_trained': 891708},\n",
       " 'done': False,\n",
       " 'episodes_total': 16,\n",
       " 'training_iteration': 1,\n",
       " 'trial_id': 'default',\n",
       " 'experiment_id': '995f84f77c3b4dbebfb158fe1da1c874',\n",
       " 'date': '2022-10-24_17-56-15',\n",
       " 'timestamp': 1666626975,\n",
       " 'time_this_iter_s': 288.0703070163727,\n",
       " 'time_total_s': 288.0703070163727,\n",
       " 'pid': 25026,\n",
       " 'hostname': 'Christophs-MBP',\n",
       " 'node_ip': '127.0.0.1',\n",
       " 'config': {'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_gpus': 0,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'placement_strategy': 'PACK',\n",
       "  'eager_tracing': False,\n",
       "  'eager_max_retraces': 20,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'env': ray.rllib.examples.env.cartpole_sparse_rewards.CartPoleSparseRewards,\n",
       "  'env_config': {},\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  'disable_env_checking': False,\n",
       "  'num_workers': 2,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'sample_async': False,\n",
       "  'enable_connectors': False,\n",
       "  'rollout_fragment_length': 200,\n",
       "  'batch_mode': 'complete_episodes',\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'validate_workers_after_construction': True,\n",
       "  'ignore_worker_failures': False,\n",
       "  'recreate_failed_workers': False,\n",
       "  'restart_failed_sub_environments': False,\n",
       "  'num_consecutive_worker_failures_tolerance': 100,\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'compress_observations': False,\n",
       "  'enable_tf1_exec_eagerly': False,\n",
       "  'sampler_perf_stats_ema_coef': None,\n",
       "  'gamma': 0.99,\n",
       "  'lr': 5e-05,\n",
       "  'train_batch_size': 4000,\n",
       "  'model': {'_use_default_native_models': False,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': True,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': ray.rllib.algorithms.alpha_zero.models.custom_torch_models.DenseModel,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'lstm_use_prev_action_reward': -1},\n",
       "  'optimizer': {},\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_config': {},\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_sample_timeout_s': 180.0,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'evaluation_config': {'extra_python_environs_for_driver': {},\n",
       "   'extra_python_environs_for_worker': {},\n",
       "   'num_gpus': 0,\n",
       "   'num_cpus_per_worker': 1,\n",
       "   'num_gpus_per_worker': 0,\n",
       "   '_fake_gpus': False,\n",
       "   'custom_resources_per_worker': {},\n",
       "   'placement_strategy': 'PACK',\n",
       "   'eager_tracing': False,\n",
       "   'eager_max_retraces': 20,\n",
       "   'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "    'inter_op_parallelism_threads': 2,\n",
       "    'gpu_options': {'allow_growth': True},\n",
       "    'log_device_placement': False,\n",
       "    'device_count': {'CPU': 1},\n",
       "    'allow_soft_placement': True},\n",
       "   'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "    'inter_op_parallelism_threads': 8},\n",
       "   'env': ray.rllib.examples.env.cartpole_sparse_rewards.CartPoleSparseRewards,\n",
       "   'env_config': {},\n",
       "   'observation_space': None,\n",
       "   'action_space': None,\n",
       "   'env_task_fn': None,\n",
       "   'render_env': False,\n",
       "   'clip_rewards': None,\n",
       "   'normalize_actions': True,\n",
       "   'clip_actions': False,\n",
       "   'disable_env_checking': False,\n",
       "   'num_workers': 2,\n",
       "   'num_envs_per_worker': 1,\n",
       "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "   'sample_async': False,\n",
       "   'enable_connectors': False,\n",
       "   'rollout_fragment_length': 200,\n",
       "   'batch_mode': 'complete_episodes',\n",
       "   'remote_worker_envs': False,\n",
       "   'remote_env_batch_wait_ms': 0,\n",
       "   'validate_workers_after_construction': True,\n",
       "   'ignore_worker_failures': False,\n",
       "   'recreate_failed_workers': False,\n",
       "   'restart_failed_sub_environments': False,\n",
       "   'num_consecutive_worker_failures_tolerance': 100,\n",
       "   'horizon': None,\n",
       "   'soft_horizon': False,\n",
       "   'no_done_at_end': False,\n",
       "   'preprocessor_pref': 'deepmind',\n",
       "   'observation_filter': 'NoFilter',\n",
       "   'synchronize_filters': True,\n",
       "   'compress_observations': False,\n",
       "   'enable_tf1_exec_eagerly': False,\n",
       "   'sampler_perf_stats_ema_coef': None,\n",
       "   'gamma': 0.99,\n",
       "   'lr': 5e-05,\n",
       "   'train_batch_size': 4000,\n",
       "   'model': {'_use_default_native_models': False,\n",
       "    '_disable_preprocessor_api': False,\n",
       "    '_disable_action_flattening': False,\n",
       "    'fcnet_hiddens': [256, 256],\n",
       "    'fcnet_activation': 'tanh',\n",
       "    'conv_filters': None,\n",
       "    'conv_activation': 'relu',\n",
       "    'post_fcnet_hiddens': [],\n",
       "    'post_fcnet_activation': 'relu',\n",
       "    'free_log_std': False,\n",
       "    'no_final_linear': False,\n",
       "    'vf_share_layers': True,\n",
       "    'use_lstm': False,\n",
       "    'max_seq_len': 20,\n",
       "    'lstm_cell_size': 256,\n",
       "    'lstm_use_prev_action': False,\n",
       "    'lstm_use_prev_reward': False,\n",
       "    '_time_major': False,\n",
       "    'use_attention': False,\n",
       "    'attention_num_transformer_units': 1,\n",
       "    'attention_dim': 64,\n",
       "    'attention_num_heads': 1,\n",
       "    'attention_head_dim': 32,\n",
       "    'attention_memory_inference': 50,\n",
       "    'attention_memory_training': 50,\n",
       "    'attention_position_wise_mlp_dim': 32,\n",
       "    'attention_init_gru_gate_bias': 2.0,\n",
       "    'attention_use_n_prev_actions': 0,\n",
       "    'attention_use_n_prev_rewards': 0,\n",
       "    'framestack': True,\n",
       "    'dim': 84,\n",
       "    'grayscale': False,\n",
       "    'zero_mean': True,\n",
       "    'custom_model': ray.rllib.algorithms.alpha_zero.models.custom_torch_models.DenseModel,\n",
       "    'custom_model_config': {},\n",
       "    'custom_action_dist': None,\n",
       "    'custom_preprocessor': None,\n",
       "    'lstm_use_prev_action_reward': -1},\n",
       "   'optimizer': {},\n",
       "   'explore': True,\n",
       "   'exploration_config': {'type': 'StochasticSampling'},\n",
       "   'input_config': {},\n",
       "   'actions_in_input_normalized': False,\n",
       "   'postprocess_inputs': False,\n",
       "   'shuffle_buffer_size': 0,\n",
       "   'output': None,\n",
       "   'output_config': {},\n",
       "   'output_compress_columns': ['obs', 'new_obs'],\n",
       "   'output_max_file_size': 67108864,\n",
       "   'evaluation_interval': None,\n",
       "   'evaluation_duration': 10,\n",
       "   'evaluation_duration_unit': 'episodes',\n",
       "   'evaluation_sample_timeout_s': 180.0,\n",
       "   'evaluation_parallel_to_training': False,\n",
       "   'evaluation_config': {'mcts_config': {'argmax_tree_policy': True,\n",
       "     'add_dirichlet_noise': False}},\n",
       "   'off_policy_estimation_methods': {},\n",
       "   'evaluation_num_workers': 0,\n",
       "   'always_attach_evaluation_results': False,\n",
       "   'enable_async_evaluation': False,\n",
       "   'in_evaluation': False,\n",
       "   'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
       "   'keep_per_episode_custom_metrics': False,\n",
       "   'metrics_episode_collection_timeout_s': 60.0,\n",
       "   'metrics_num_episodes_for_smoothing': 100,\n",
       "   'min_time_s_per_iteration': None,\n",
       "   'min_train_timesteps_per_iteration': 0,\n",
       "   'min_sample_timesteps_per_iteration': 0,\n",
       "   'logger_creator': None,\n",
       "   'logger_config': None,\n",
       "   'log_level': 'WARN',\n",
       "   'log_sys_usage': True,\n",
       "   'fake_sampler': False,\n",
       "   'seed': None,\n",
       "   '_tf_policy_handles_more_than_one_loss': False,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   '_disable_execution_plan_api': True,\n",
       "   'simple_optimizer': False,\n",
       "   'monitor': -1,\n",
       "   'evaluation_num_episodes': -1,\n",
       "   'metrics_smoothing_episodes': -1,\n",
       "   'timesteps_per_iteration': -1,\n",
       "   'min_iter_time_s': -1,\n",
       "   'collect_metrics_timeout': -1,\n",
       "   'buffer_size': -1,\n",
       "   'prioritized_replay': -1,\n",
       "   'learning_starts': 1000,\n",
       "   'replay_batch_size': -1,\n",
       "   'replay_sequence_length': None,\n",
       "   'prioritized_replay_alpha': -1,\n",
       "   'prioritized_replay_beta': -1,\n",
       "   'prioritized_replay_eps': -1,\n",
       "   'min_time_s_per_reporting': -1,\n",
       "   'min_train_timesteps_per_reporting': -1,\n",
       "   'min_sample_timesteps_per_reporting': -1,\n",
       "   'input_evaluation': -1,\n",
       "   'sgd_minibatch_size': 256,\n",
       "   'shuffle_sequences': True,\n",
       "   'num_sgd_iter': 30,\n",
       "   'replay_buffer_config': {'type': ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer,\n",
       "    'capacity': 1000,\n",
       "    'learning_starts': 1000,\n",
       "    'storage_unit': 'fragments',\n",
       "    'replay_mode': 'independent'},\n",
       "   'lr_schedule': None,\n",
       "   'vf_share_layers': False,\n",
       "   'mcts_config': {'puct_coefficient': 1.0,\n",
       "    'num_simulations': 30,\n",
       "    'temperature': 1.5,\n",
       "    'dirichlet_epsilon': 0.25,\n",
       "    'dirichlet_noise': 0.03,\n",
       "    'argmax_tree_policy': True,\n",
       "    'add_dirichlet_noise': False},\n",
       "   'ranked_rewards': {'enable': True,\n",
       "    'percentile': 75,\n",
       "    'buffer_max_length': 1000,\n",
       "    'initialize_buffer': True,\n",
       "    'num_init_rewards': 100},\n",
       "   'input': 'sampler',\n",
       "   'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec at 0x7f8e42ee26a0>},\n",
       "    'policy_map_capacity': 100,\n",
       "    'policy_map_cache': None,\n",
       "    'policy_mapping_fn': None,\n",
       "    'policies_to_train': None,\n",
       "    'observation_fn': None,\n",
       "    'replay_mode': 'independent',\n",
       "    'count_steps_by': 'env_steps'},\n",
       "   'callbacks': ray.rllib.algorithms.alpha_zero.alpha_zero.AlphaZeroDefaultCallbacks,\n",
       "   'create_env_on_driver': False,\n",
       "   'custom_eval_function': None,\n",
       "   'framework': 'torch',\n",
       "   'num_cpus_for_driver': 1},\n",
       "  'off_policy_estimation_methods': {},\n",
       "  'evaluation_num_workers': 0,\n",
       "  'always_attach_evaluation_results': False,\n",
       "  'enable_async_evaluation': False,\n",
       "  'in_evaluation': False,\n",
       "  'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
       "  'keep_per_episode_custom_metrics': False,\n",
       "  'metrics_episode_collection_timeout_s': 60.0,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_iteration': None,\n",
       "  'min_train_timesteps_per_iteration': 0,\n",
       "  'min_sample_timesteps_per_iteration': 0,\n",
       "  'logger_creator': None,\n",
       "  'logger_config': None,\n",
       "  'log_level': 'WARN',\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'seed': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_execution_plan_api': True,\n",
       "  'simple_optimizer': False,\n",
       "  'monitor': -1,\n",
       "  'evaluation_num_episodes': -1,\n",
       "  'metrics_smoothing_episodes': -1,\n",
       "  'timesteps_per_iteration': -1,\n",
       "  'min_iter_time_s': -1,\n",
       "  'collect_metrics_timeout': -1,\n",
       "  'buffer_size': -1,\n",
       "  'prioritized_replay': -1,\n",
       "  'learning_starts': 1000,\n",
       "  'replay_batch_size': -1,\n",
       "  'replay_sequence_length': None,\n",
       "  'prioritized_replay_alpha': -1,\n",
       "  'prioritized_replay_beta': -1,\n",
       "  'prioritized_replay_eps': -1,\n",
       "  'min_time_s_per_reporting': -1,\n",
       "  'min_train_timesteps_per_reporting': -1,\n",
       "  'min_sample_timesteps_per_reporting': -1,\n",
       "  'input_evaluation': -1,\n",
       "  'sgd_minibatch_size': 256,\n",
       "  'shuffle_sequences': True,\n",
       "  'num_sgd_iter': 30,\n",
       "  'replay_buffer_config': {'type': ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer,\n",
       "   'capacity': 1000,\n",
       "   'learning_starts': 1000,\n",
       "   'storage_unit': 'fragments',\n",
       "   'replay_mode': 'independent'},\n",
       "  'lr_schedule': None,\n",
       "  'vf_share_layers': False,\n",
       "  'mcts_config': {'puct_coefficient': 1.0,\n",
       "   'num_simulations': 30,\n",
       "   'temperature': 1.5,\n",
       "   'dirichlet_epsilon': 0.25,\n",
       "   'dirichlet_noise': 0.03,\n",
       "   'argmax_tree_policy': False,\n",
       "   'add_dirichlet_noise': True},\n",
       "  'ranked_rewards': {'enable': True,\n",
       "   'percentile': 75,\n",
       "   'buffer_max_length': 1000,\n",
       "   'initialize_buffer': True,\n",
       "   'num_init_rewards': 100},\n",
       "  'input': 'sampler',\n",
       "  'multiagent': {'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec at 0x7f8e42ee2640>},\n",
       "   'policy_map_capacity': 100,\n",
       "   'policy_map_cache': None,\n",
       "   'policy_mapping_fn': None,\n",
       "   'policies_to_train': None,\n",
       "   'observation_fn': None,\n",
       "   'replay_mode': 'independent',\n",
       "   'count_steps_by': 'env_steps'},\n",
       "  'callbacks': ray.rllib.algorithms.alpha_zero.alpha_zero.AlphaZeroDefaultCallbacks,\n",
       "  'create_env_on_driver': False,\n",
       "  'custom_eval_function': None,\n",
       "  'framework': 'torch',\n",
       "  'num_cpus_for_driver': 1},\n",
       " 'time_since_restore': 288.0703070163727,\n",
       " 'timesteps_since_restore': 0,\n",
       " 'iterations_since_restore': 1,\n",
       " 'warmup_time': 9.681735038757324,\n",
       " 'perf': {'cpu_util_percent': 23.109535452322735,\n",
       "  'ram_util_percent': 66.23520782396088}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from ray.rllib.algorithms.alpha_zero import AlphaZeroConfig\n",
    "from ray.rllib.algorithms.alpha_zero.models.custom_torch_models import DenseModel\n",
    "from ray.rllib.examples.env.cartpole_sparse_rewards import CartPoleSparseRewards\n",
    "from ray.rllib.models.catalog import ModelCatalog\n",
    "\n",
    "ModelCatalog.register_custom_model(\"dense_model\", DenseModel)\n",
    "\n",
    "def env_creator(config):\n",
    "    env=CartPoleSparseRewards(\"CartPole-v1\")\n",
    "    return env \n",
    "\n",
    "\n",
    "# use tune to register the custom environment for the ppo trainer\n",
    "#tune.register_env('Cart',env_creator)\n",
    "\n",
    "config = AlphaZeroConfig().training(sgd_minibatch_size=256)\\\n",
    "            .resources(num_gpus=0)\\\n",
    "            .environment(env=CartPoleSparseRewards)\\\n",
    "            .training(model={\"custom_model\": DenseModel})\n",
    "print(config.to_dict())\n",
    "# Build a Algorithm object from the config and run 1 training iteration.\n",
    "\n",
    "\n",
    "trainer = config.build()\n",
    "print(\"start training\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.alpha_zero import AlphaZeroConfig\n",
    "from ray import tune\n",
    "config = AlphaZeroConfig()\n",
    "# Print out some default values.\n",
    "print(config.shuffle_sequences)\n",
    "# Update the config object.\n",
    "config.training(lr=tune.grid_search([0.001, 0.0001]))\n",
    "# Set the config object's env.\n",
    "config.environment(env=\"Cart\")\n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.run(\n",
    "    \"AlphaZero\",\n",
    "    stop={\"episode_reward_mean\": 200},\n",
    "    config=config.to_dict())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\\.rollouts(num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': None, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 2, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'complete_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'mcts_config': {'argmax_tree_policy': True, 'add_dirichlet_noise': False}}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': 1000, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'sgd_minibatch_size': 256, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'replay_buffer_config': {'type': 'ReplayBuffer', 'capacity': 1000, 'learning_starts': 500, 'storage_unit': 'fragments'}, 'lr_schedule': None, 'vf_share_layers': False, 'mcts_config': {'puct_coefficient': 1.0, 'num_simulations': 30, 'temperature': 1.5, 'dirichlet_epsilon': 0.25, 'dirichlet_noise': 0.03, 'argmax_tree_policy': False, 'add_dirichlet_noise': True}, 'ranked_rewards': {'enable': True, 'percentile': 75, 'buffer_max_length': 1000, 'initialize_buffer': True, 'num_init_rewards': 100}, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.alpha_zero.alpha_zero.AlphaZeroDefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('cleaninstall')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc7292e1960732d9c4c32925edefe52037f482083a9087ff002fef268bf3b5aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
