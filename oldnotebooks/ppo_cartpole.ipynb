{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-24 20:11:44,821\tWARNING trainer.py:2540 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-08-24 20:11:55,718\tWARNING deprecation.py:46 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-08-24 20:11:55,719\tWARNING trainer.py:2540 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-08-24 20:11:55,781\tINFO trainable.py:159 -- Trainable.setup took 10.966 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    }
   ],
   "source": [
    "# tip:\n",
    "#b) To register your custom env, do `from ray import tune;\n",
    "#   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
    "#   Then in your config, do `config['env'] = [name]`.\n",
    "\n",
    "# Import the RL algorithm (Trainer) we would like to use.\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.contrib.alpha_zero.core.alpha_zero_trainer import AlphaZeroTrainer\n",
    "#import src.jss_graph_env.disjunctive_graph_jss_env as jss_env\n",
    "#import src.jsp_instance_parser \n",
    "from ray import tune\n",
    "import gym\n",
    "\n",
    "from ray import tune\n",
    "from wrapper.taxi_wrapper import discretetobox\n",
    "\n",
    "# Configure the algorithm.\n",
    "def env_creator(env_config):\n",
    "    env = (gym.make(\"Taxi-v3\"))\n",
    "    #env = discretetobox(gym.make(\"Taxi-v3\"))\n",
    "\n",
    "    #env = gym.make('LunarLander-v2')\n",
    "    return env\n",
    "\n",
    "# use tune to register the custom environment for the ppo trainer\n",
    "tune.register_env('TaxiTaxi',env_creator)\n",
    "\n",
    "\n",
    "config = {\n",
    "    # Environment (RLlib understands openAI gym registered strings).\n",
    "    #\"env\": \"CartPole-v1\",\n",
    "    \"env\": \"Taxi-v3\",\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 1,\n",
    "    # Change this to \"framework: torch\", if you are using PyTorch.\n",
    "    # Also, use \"framework: tf2\" for tf2.x eager execution.\n",
    "    \"framework\": \"tf\",\n",
    "    # Tweak the default model provided automatically by RLlib,\n",
    "    # given the environment's observation- and action spaces.\n",
    "    #\"horizon\":1,\n",
    "    \"evaluation_duration\":10,\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [64, 64],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    # Set up a separate evaluation worker set for the\n",
    "    # `trainer.evaluate()` call after training (see below).\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    # Only for evaluation runs, render the env.\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": True,\n",
    "    },\n",
    "    \n",
    "}\n",
    "\n",
    "# function which returns environment, by now config is unused...\n",
    "\n",
    "\n",
    "# use tune to register the custom environment for the ppo trainer\n",
    "#tune.register_env('CartPole-v1',env_creator)\n",
    "#tune.register_env('Taxi-v3',env_creator)\n",
    "# Create our RLlib Trainer.\n",
    "trainer = PPOTrainer(config=config)\n",
    "\n",
    "#trainer = AlphaZeroTrainer(config=config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-24 20:10:49,561\tWARNING deprecation.py:46 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer trained: 1 Epochs\n",
      "trainer trained: 2 Epochs\n",
      "trainer trained: 3 Epochs\n",
      "trainer trained: 4 Epochs\n",
      "trainer trained: 5 Epochs\n",
      "trainer trained: 6 Epochs\n",
      "trainer trained: 7 Epochs\n",
      "trainer trained: 8 Epochs\n",
      "trainer trained: 9 Epochs\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from ray import tune\n",
    "#path='resources/jsp_instances/standard/abz8.txt'\n",
    "\n",
    "#env = gym.make('CartPole-v1')\n",
    "#env = gym.make('Taxi-v3')\n",
    "\n",
    "#env = gym.make('LunarL')\n",
    "#env.render()\n",
    "# run until episode ends\n",
    "# list that lists all actions\n",
    "iteration_list=[]\n",
    "epoch_nr=0\n",
    "for _ in range(300):\n",
    "    epoch_nr+=1\n",
    "    trainer.train()\n",
    "    print(f\"trainer trained: {epoch_nr} Epochs\")\n",
    "    #trainer.save(f\"checkpoints/rllib_checkpoint{_+1}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use string number to restore\n",
    "#nr_restore=\"50\"\n",
    "#trainer.restore(f'checkpoints/rllib_checkpoint{nr_restore}/checkpoint_{nr_restore.zfill(6)}/checkpoint-{nr_restore}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "-290\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.contrib.alpha_zero.environments.cartpole import CartPole\n",
    "#env = gym.make('CartPole-v1')\n",
    "#env = discretetobox(gym.make(\"Taxi-v3\"))\n",
    "episode_reward = 0\n",
    "env = (gym.make(\"Taxi-v3\"))\n",
    "\n",
    "done = False\n",
    "obs = env.reset()\n",
    "iterations=0\n",
    "while not done:\n",
    "    action = trainer.compute_action(obs)\n",
    "    #print(f\"action: {action}\")\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "    #env.render(mode=\"human\")\n",
    "    iterations += 1\n",
    "env.render()\n",
    "env.close()\n",
    "print(episode_reward)\n",
    "print(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-24 20:11:55,826\tWARNING trainer.py:2540 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-08-24 20:12:06,487\tWARNING deprecation.py:46 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-08-24 20:12:06,489\tWARNING trainer.py:2540 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-08-24 20:12:06,594\tINFO trainable.py:159 -- Trainable.setup took 10.771 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-08-24 20:12:10,536\tWARNING deprecation.py:46 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: Min/Mean/Max reward: -875.0000/-755.2500/-457.0000, len mean: 193.0500. \n",
      "  2: Min/Mean/Max reward: -929.0000/-766.2500/-457.0000, len mean: 196.1000. \n",
      "  3: Min/Mean/Max reward: -929.0000/-735.4098/-256.0000, len mean: 195.3115. \n",
      "  4: Min/Mean/Max reward: -929.0000/-697.3415/-69.0000, len mean: 194.5122. \n",
      "  5: Min/Mean/Max reward: -929.0000/-653.6300/-69.0000, len mean: 195.5000. \n",
      "  6: Min/Mean/Max reward: -929.0000/-586.5000/-69.0000, len mean: 196.8900. \n",
      "  7: Min/Mean/Max reward: -776.0000/-504.7600/-69.0000, len mean: 195.9700. \n",
      "  8: Min/Mean/Max reward: -695.0000/-434.7200/-69.0000, len mean: 197.2700. \n",
      "  9: Min/Mean/Max reward: -569.0000/-375.8200/-151.0000, len mean: 198.3100. \n",
      " 10: Min/Mean/Max reward: -497.0000/-334.3300/-151.0000, len mean: 198.3100. \n"
     ]
    }
   ],
   "source": [
    "SELECT_ENV = \"Taxi-v3\"\n",
    "N_ITER = 10\n",
    "config[\"log_level\"] = \"WARN\"\n",
    "\n",
    "agent = trainer = PPOTrainer(config=config)\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "for n in range(N_ITER):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    \n",
    "    episode = {'n': n, \n",
    "               'episode_reward_min': result['episode_reward_min'], \n",
    "               'episode_reward_mean': result['episode_reward_mean'], \n",
    "               'episode_reward_max': result['episode_reward_max'],  \n",
    "               'episode_len_mean': result['episode_len_mean']\n",
    "              }\n",
    "    \n",
    "    episode_data.append(episode)\n",
    "    \n",
    "    print(f'{n+1:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}, len mean: {result[\"episode_len_mean\"]:8.4f}. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import ray\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.contrib.alpha_zero.models.custom_torch_models import DenseModel\n",
    "from ray.rllib.contrib.alpha_zero.environments.cartpole import CartPole\n",
    "from ray.rllib.contrib.alpha_zero.core.alpha_zero_trainer import AlphaZeroTrainer\n",
    "from ray.rllib.models.catalog import ModelCatalog\n",
    "\n",
    "\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\"--training-iteration\", default=2, type=int)\n",
    "#args = parser.parse_args()\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "ModelCatalog.register_custom_model(\"dense_model\", DenseModel)\n",
    "register_env(\"CartPoleEnv\", lambda _: CartPole())\n",
    "\n",
    "config = {\n",
    "    \"num_workers\"       : 0,\n",
    "    \"rollout_fragment_length\": 50,\n",
    "    \"train_batch_size\"  : 500,\n",
    "    \"sgd_minibatch_size\": 64,\n",
    "    \"lr\"                : 1e-4,\n",
    "    \"num_sgd_iter\"      : 1,\n",
    "    \"mcts_config\"       : {\n",
    "        \"puct_coefficient\"   : 1.5,\n",
    "        \"num_simulations\"    : 100,\n",
    "        \"temperature\"        : 1.0,\n",
    "        \"dirichlet_epsilon\"  : 0.20,\n",
    "        \"dirichlet_noise\"    : 0.03,\n",
    "        \"argmax_tree_policy\" : False,\n",
    "        \"add_dirichlet_noise\": True,\n",
    "    },\n",
    "    \"ranked_rewards\"    : {\n",
    "        \"enable\": True,\n",
    "    },\n",
    "    \"model\"             : {\n",
    "        \"custom_model\": \"dense_model\",\n",
    "    },\n",
    "}\n",
    "\n",
    "agent = AlphaZeroTrainer(config=config, env=\"CartPoleEnv\")\n",
    "\n",
    "#for _ in range(args.training_iteration):\n",
    "for _ in range(1):\n",
    "    agent.train()\n",
    "\n",
    "env = CartPole()\n",
    "episode_reward = 0\n",
    "done = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "while not done:\n",
    "    print(obs)\n",
    "    action = agent.compute_single_action(obs)\n",
    "    obs, episode_reward, done, info = env.step(action)\n",
    "\n",
    "print(episode_reward)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import rllib, tune\n",
    "from ray.rllib.contrib.alpha_zero.core.alpha_zero_trainer import AlphaZeroTrainer\n",
    "from ray.rllib.policy.sample_batch import DEFAULT_POLICY_ID\n",
    "from ray.rllib.policy.policy_map import PolicyMap\n",
    "from ray.rllib.evaluation.episode import MultiAgentEpisode\n",
    "\n",
    "\n",
    "\n",
    "#agent = AlphaZeroTrainer(env=CartPole, config=config)\n",
    "agent = AlphaZeroTrainer(env=CartPole, config=config)\n",
    "for _ in range(1):\n",
    "    agent.train()\n",
    "\n",
    "policy = agent.get_policy(DEFAULT_POLICY_ID)\n",
    "\n",
    "env = CartPole()\n",
    "obs = env.reset()\n",
    "\n",
    "episode = MultiAgentEpisode(\n",
    "    PolicyMap(0,0),\n",
    "    lambda _, __: DEFAULT_POLICY_ID,\n",
    "    lambda: None,\n",
    "    lambda _: None,\n",
    "    0,\n",
    ")\n",
    "\n",
    "episode.user_data['initial_state'] = env.get_state()\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action, _, _ = policy.compute_single_action(obs, episode=episode)\n",
    "    print(action)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    print(obs, reward)\n",
    "    episode.length += 1\n",
    "\n",
    "assert reward == episode.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "policy = agent.get_policy(DEFAULT_POLICY_ID)\n",
    "\n",
    "env = CartPole()\n",
    "obs = env.reset()\n",
    "\n",
    "episode = MultiAgentEpisode(\n",
    "    PolicyMap(0,0),\n",
    "    lambda _, __: DEFAULT_POLICY_ID,\n",
    "    lambda: None,\n",
    "    lambda _: None,\n",
    "    0,\n",
    ")\n",
    "\n",
    "episode.user_data['initial_state'] = env.get_state()\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action, _, _ = policy.compute_single_action(obs, episode=episode)\n",
    "    print(action)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    episode.length += 1\n",
    "\n",
    "assert reward == episode.length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('cleaninstall')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc7292e1960732d9c4c32925edefe52037f482083a9087ff002fef268bf3b5aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
