{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tip:\n",
    "#b) To register your custom env, do `from ray import tune;\n",
    "#   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
    "#   Then in your config, do `config['env'] = [name]`.\n",
    "\n",
    "# Import the RL algorithm (Trainer) we would like to use.\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "import src.jss_graph_env.disjunctive_graph_jss_env as jss_env\n",
    "import src.jsp_instance_parser \n",
    "from ray import tune\n",
    "\n",
    "# Configure the algorithm.\n",
    "config = {\n",
    "    # Environment (RLlib understands openAI gym registered strings).\n",
    "    \"env\": \"jss_env\",\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 1,\n",
    "    # Change this to \"framework: torch\", if you are using PyTorch.\n",
    "    # Also, use \"framework: tf2\" for tf2.x eager execution.\n",
    "    \"framework\": \"tf\",\n",
    "    # Tweak the default model provided automatically by RLlib,\n",
    "    # given the environment's observation- and action spaces.\n",
    "    #\"horizon\":1,\n",
    "    \"evaluation_duration\":10,\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [64, 64],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    # Set up a separate evaluation worker set for the\n",
    "    # `trainer.evaluate()` call after training (see below).\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    # Only for evaluation runs, render the env.\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "# function which returns environment, by now config is unused...\n",
    "def env_creator(env_config):\n",
    "    path='resources/jsp_instances/standard/la01.txt'\n",
    "    curr_instance=src.jsp_instance_parser.parse_jps_standard_specification(path)\n",
    "    res,std_matrix=curr_instance\n",
    "    env = jss_env.DisjunctiveGraphJssEnv(res,default_visualisations='gantt_console',reward_mode='utilisation')\n",
    "    return env\n",
    "\n",
    "# use tune to register the custom environment for the ppo trainer\n",
    "tune.register_env('jss_env',env_creator)\n",
    "\n",
    "# Create our RLlib Trainer.\n",
    "trainer = PPOTrainer(config=config)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it for n training iterations. A training iteration includes\n",
    "# parallel sample collection by the environment workers as well as\n",
    "# loss calculation on the collected batch and a model update.\n",
    "#import time\n",
    "#for _ in range(10):\n",
    "#   tmp=time.time()\n",
    "#   print(f\"training nr. {_}\")\n",
    "#   trainer.train()\n",
    "#   print(f\"training for iteration {_} took {time.time()-tmp} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "path='resources/jsp_instances/standard/la01.txt'\n",
    "#path='resources/jsp_instances/standard/abz8.txt'\n",
    "curr_instance=src.jsp_instance_parser.parse_jps_standard_specification(path)\n",
    "res,std_matrix=curr_instance\n",
    "env = jss_env.DisjunctiveGraphJssEnv(res,default_visualisations='gantt_console')\n",
    "#env.render()\n",
    "# run until episode ends\n",
    "# list that lists all actions\n",
    "action_list=np.arange(0,env.n_jobs*env.n_machines)\n",
    "iteration_list=[]\n",
    "invalid_action_list=[]\n",
    "finished_list=[]\n",
    "epoch_nr=0\n",
    "for _ in range(10):\n",
    "    epoch_nr+=1\n",
    "    trainer.train()\n",
    "    print(f\"trainer trained: {epoch_nr} Epochs\")\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    iteration=0\n",
    "    invalid_action=0\n",
    "\n",
    "    while not done or iteration>5000:\n",
    "        iteration +=1\n",
    "        action = trainer.compute_action(obs)\n",
    "        if action not in action_list[env.valid_action_mask()]: invalid_action+=1\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "    print(f\"scheduling finished in {iteration} iterations with {invalid_action} invalid actions\")\n",
    "    env.render()\n",
    "\n",
    "    data_df=env.network_as_dataframe()\n",
    "\n",
    "    finished_list.append(max(data_df[\"Finish\"]))\n",
    "    iteration_list.append(iteration)\n",
    "    invalid_action_list.append(invalid_action)\n",
    "\n",
    "plt.plot(finished_list)\n",
    "plt.title(\"finish time\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(iteration_list)\n",
    "plt.title(\"count of iterations\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(invalid_action_list)\n",
    "plt.title(\"count of invalid actions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    epoch_nr+=1\n",
    "    trainer.train()\n",
    "    print(f\"trainer trained: {epoch_nr} Epochs\")\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    iteration=0\n",
    "    invalid_action=0\n",
    "\n",
    "    while not done or iteration<5000:\n",
    "        iteration +=1\n",
    "        action = trainer.compute_action(obs)\n",
    "        if action not in action_list[env.valid_action_mask()]: invalid_action+=1\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "    print(f\"scheduling finished in {iteration} iterations with {invalid_action} invalid actions\")\n",
    "    env.render()\n",
    "\n",
    "    data_df=env.network_as_dataframe()\n",
    "\n",
    "    finished_list.append(max(data_df[\"Finish\"]))\n",
    "    iteration_list.append(iteration)\n",
    "    invalid_action_list.append(invalid_action)\n",
    "\n",
    "plt.plot(finished_list)\n",
    "plt.title(\"finish time\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(iteration_list)\n",
    "plt.title(\"count of iterations\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(invalid_action_list)\n",
    "plt.title(\"count of invalid actions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    epoch_nr+=1\n",
    "    trainer.train()\n",
    "    print(f\"trainer trained: {epoch_nr} Epochs\")\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    iteration=0\n",
    "    invalid_action=0\n",
    "\n",
    "    while not done:\n",
    "        iteration +=1\n",
    "        action = trainer.compute_action(obs)\n",
    "        if action not in action_list[env.valid_action_mask()]: invalid_action+=1\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "    print(f\"scheduling finished in {iteration} iterations with {invalid_action} invalid actions\")\n",
    "    env.render()\n",
    "\n",
    "    data_df=env.network_as_dataframe()\n",
    "\n",
    "    finished_list.append(max(data_df[\"Finish\"]))\n",
    "    iteration_list.append(iteration)\n",
    "    invalid_action_list.append(invalid_action)\n",
    "\n",
    "plt.plot(finished_list)\n",
    "plt.title(\"finish time\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(iteration_list)\n",
    "plt.title(\"count of iterations\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(invalid_action_list)\n",
    "plt.title(\"count of invalid actions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    epoch_nr+=1\n",
    "    trainer.train()\n",
    "    print(f\"trainer trained: {epoch_nr} Epochs\")\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    iteration=0\n",
    "    invalid_action=0\n",
    "\n",
    "    while not done:\n",
    "        iteration +=1\n",
    "        action = trainer.compute_action(obs)\n",
    "        if action not in action_list[env.valid_action_mask()]: invalid_action+=1\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "    print(f\"scheduling finished in {iteration} iterations with {invalid_action} invalid actions\")\n",
    "    env.render()\n",
    "\n",
    "    data_df=env.network_as_dataframe()\n",
    "\n",
    "    finished_list.append(max(data_df[\"Finish\"]))\n",
    "    iteration_list.append(iteration)\n",
    "    invalid_action_list.append(invalid_action)\n",
    "\n",
    "plt.plot(finished_list)\n",
    "plt.title(\"finish time\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(iteration_list)\n",
    "plt.title(\"count of iterations\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(invalid_action_list)\n",
    "plt.title(\"count of invalid actions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    epoch_nr+=1\n",
    "    trainer.train()\n",
    "    print(f\"trainer trained: {epoch_nr} Epochs\")\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    iteration=0\n",
    "    invalid_action=0\n",
    "\n",
    "    while not done:\n",
    "        iteration +=1\n",
    "        action = trainer.compute_action(obs)\n",
    "        if action not in action_list[env.valid_action_mask()]: invalid_action+=1\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "    print(f\"scheduling finished in {iteration} iterations with {invalid_action} invalid actions\")\n",
    "    env.render()\n",
    "\n",
    "    data_df=env.network_as_dataframe()\n",
    "\n",
    "    finished_list.append(max(data_df[\"Finish\"]))\n",
    "    iteration_list.append(iteration)\n",
    "    invalid_action_list.append(invalid_action)\n",
    "\n",
    "plt.plot(finished_list)\n",
    "plt.title(\"finish time\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(iteration_list)\n",
    "plt.title(\"count of iterations\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(invalid_action_list)\n",
    "plt.title(\"count of invalid actions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    epoch_nr+=1\n",
    "    trainer.train()\n",
    "    print(f\"trainer trained: {epoch_nr} Epochs\")\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    iteration=0\n",
    "    invalid_action=0\n",
    "\n",
    "    while not done:\n",
    "        iteration +=1\n",
    "        action = trainer.compute_action(obs)\n",
    "        if action not in action_list[env.valid_action_mask()]: invalid_action+=1\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "    print(f\"scheduling finished in {iteration} iterations with {invalid_action} invalid actions\")\n",
    "    env.render()\n",
    "\n",
    "    data_df=env.network_as_dataframe()\n",
    "\n",
    "    finished_list.append(max(data_df[\"Finish\"]))\n",
    "    iteration_list.append(iteration)\n",
    "    invalid_action_list.append(invalid_action)\n",
    "\n",
    "plt.plot(finished_list)\n",
    "plt.title(\"finish time\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(iteration_list)\n",
    "plt.title(\"count of iterations\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(invalid_action_list)\n",
    "plt.title(\"count of invalid actions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df=env.network_as_dataframe()\n",
    "print(max(data_df[\"Finish\"]))\n",
    "\n",
    "plt.plot(finished_list)\n",
    "plt.title(\"finished after iterations\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(iteration_list)\n",
    "plt.title(\"iterations\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(invalid_action_list)\n",
    "plt.title(\"number of illegal actions\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not know what render does\n",
    "# Evaluate the trained Trainer (and rendxer each timestep to the shell's\n",
    "# output).\n",
    "#evaluate(\n",
    "    #    self,\n",
    "   #     episodes_left_fn=None,  # deprecated\n",
    "  #      duration_fn: Optional[Callable[[int], int]] = None,\n",
    " #   ) \n",
    "#print('start evaluate')\n",
    "#trainer.evaluate()\n",
    "#   self.get_policy(policy_id) and call compute_actions()\n",
    "#trainer.get_policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
