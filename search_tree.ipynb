{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper\n",
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper_multi_instances\n",
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper_no_action_mask\n",
    "from src.jss_lite.jss_lite import jss_lite\n",
    "import numpy as np\n",
    "import time\n",
    "class tree_node:\n",
    "    def __init__(self, action=None,parent=None):\n",
    "        self.action=action\n",
    "        self.children=[]\n",
    "        self.state=None\n",
    "        self.parent=parent\n",
    "        self.observation=None\n",
    "        self.visited=False\n",
    "        # extra attributes for search:\n",
    "        self.leaf_closed=False\n",
    "class tree_construct:\n",
    "    def __init__(self):\n",
    "        # threshold for values that are as good as the threshold*max_policy\n",
    "        self.threshold=0.7\n",
    "        curr_dir='/Users/felix/sciebo/masterarbeit/progra/model-based_rl'\n",
    "        inst=curr_dir + '/resources/jsp_instances/standard/la01.txt'\n",
    "        instance_list=['/resources/jsp_instances/standard/la01.txt']#,'/resources/jsp_instances/standard/la02.txt','/resources/jsp_instances/standard/la03.txt','/resources/jsp_instances/standard/la04.txt']\n",
    "        #self.env=jssp_light_obs_wrapper_multi_instances(instances_list=[inst])\n",
    "        self.env=jssp_light_obs_wrapper_no_action_mask(jssp_light_obs_wrapper_multi_instances(instances_list=[inst]))\n",
    "        # create root\n",
    "        self.root=tree_node()\n",
    "        # get initial state into root; todo: change for multi instances, create all childs for instances\n",
    "        self.root.observation=self.env.reset()\n",
    "        self.root.state=self.env.get_state()\n",
    "        self.done=False\n",
    "        self.reward=0\n",
    "        self.trainer=self.load_trainer()\n",
    "        # path management\n",
    "        # open leafs should store the leafs with highest rating\n",
    "        self.open_leafs=[]\n",
    "        self.open_leafs.append(self.root)\n",
    "        self.current_path=self.root\n",
    "        self.sol_pathes=None\n",
    "    def insert(self,action=None,parent=None):\n",
    "        parent.children.append(tree_node(action=action,parent=self))\n",
    "        self.env.set_state(parent.state)\n",
    "        parent.children[-1].observation,reward,done,info=self.env.step(action)\n",
    "        parent.children[-1].state=self.env.get_state()\n",
    "        if done:\n",
    "            self.__on_done(reward)\n",
    "\n",
    "    def __on_done(self,reward):\n",
    "        self.done=True\n",
    "        if reward > self.reward:\n",
    "            self.reward=reward\n",
    "    def load_trainer(self):\n",
    "        from ray.rllib.agents.ppo import PPOTrainer\n",
    "        from ray import tune\n",
    "        import gym\n",
    "        from wrapper.jssplight_wrapper import jssp_light_obs_wrapper\n",
    "        from wrapper.jssplight_wrapper import jssp_light_obs_wrapper_multi_instances\n",
    "        from wrapper.jssplight_wrapper import jssp_light_obs_wrapper_no_action_mask\n",
    "        from src.jss_lite.jss_lite import jss_lite\n",
    "        from ray import tune\n",
    "        curr_dir='/Users/felix/sciebo/masterarbeit/progra/model-based_rl'\n",
    "        inst=curr_dir + '/resources/jsp_instances/standard/la01.txt'\n",
    "        instance_list=['/resources/jsp_instances/standard/la01.txt']#,'/resources/jsp_instances/standard/la02.txt','/resources/jsp_instances/standard/la03.txt','/resources/jsp_instances/standard/la04.txt']\n",
    "        # Configure the algorithm.\n",
    "        def env_creator(env_config):\n",
    "            #env=jssp_light_obs_wrapper_multi_instances(instances_list=[inst])\n",
    "            env=jssp_light_obs_wrapper_no_action_mask(jssp_light_obs_wrapper_multi_instances(instances_list=[inst]))\n",
    "            #env=jss_lite(instance_path=inst)\n",
    "            #env=jssp_light_obs_wrapper_no_action_mask(jss_lite(instance_path=inst))\n",
    "            return env\n",
    "        tune.register_env('custom_jssp',env_creator)\n",
    "        config = {\n",
    "            \"env\": \"custom_jssp\",\n",
    "            \"disable_env_checking\":True,\n",
    "            \"num_workers\": 4,\n",
    "            \"framework\": \"tf\",\n",
    "            \"model\": {\n",
    "                \"fcnet_hiddens\": [64, 64],\n",
    "                \"fcnet_activation\": \"relu\",\n",
    "            },\n",
    "            \"evaluation_num_workers\": 0,\n",
    "            \"evaluation_config\": {\n",
    "                \"render_env\": False,\n",
    "            },\n",
    "            \n",
    "        }\n",
    "        trainer = PPOTrainer(config=config)\n",
    "        trainer.load_checkpoint('/Users/felix/sciebo/masterarbeit/progra/model-based_rl/ppo/checkpoint-4')\n",
    "        return trainer\n",
    "\n",
    "    def propose_actions(self,state,observation):\n",
    "        policy = self.trainer.get_policy()\n",
    "        action, _, info = policy.compute_single_action(observation)\n",
    "        logits = info['action_dist_inputs']\n",
    "        self.env.set_state(state=state)\n",
    "        action_mask = self.env.get_legal_actions().astype(np.float32)\n",
    "        action_mask[action_mask==0] = - np.inf\n",
    "        probs=np.multiply(logits,action_mask)\n",
    "        probs[probs==np.inf]=-np.inf\n",
    "        best_action_id = probs.argmax()  # deterministic\n",
    "        highest_prob=probs.max()\n",
    "        #todo: check if it is good here\n",
    "        out,=np.where(probs>self.threshold*highest_prob)\n",
    "        return out, best_action_id    \n",
    "\n",
    "\n",
    "    def solve(self,max_pathes=None,max_time=None):\n",
    "        # here comes the solving method with abort conditions\n",
    "        start_time=time.time()\n",
    "        p_time=time.time()\n",
    "        self.sol_pathes=0\n",
    "\n",
    "        if max_pathes==None:\n",
    "            max_pathes=1000 \n",
    "        if max_time==None:\n",
    "            max_time=60\n",
    "        #print(self.open_leafs)\n",
    "        while self.open_leafs:\n",
    "            self.sol_pathes+=1\n",
    "            # abort conditions:\n",
    "            node=self.open_leafs.pop()\n",
    "            if self.sol_pathes > max_pathes or time.time()-start_time > max_time:\n",
    "                return False\n",
    "            while not self.done:\n",
    "                loop=0\n",
    "                \n",
    "                #possible_actions=np.random.choice(len(node.observation['action_mask']), p_r, p=(node.observation['action_mask'] / node.observation['action_mask'].sum()))\n",
    "                possible_actions, best_action = self.propose_actions(node.state,node.observation)\n",
    "                for action in set(possible_actions):\n",
    "                    if action!=best_action:\n",
    "                        #print(f\"action is: {action}\")\n",
    "                        self.insert(action=action,parent=node)\n",
    "                #assure that best action is insertet last\n",
    "                self.insert(action=best_action,parent=node)\n",
    "                for child in node.children:\n",
    "                    if child != node.children[-1]:\n",
    "                        #todo: check here if in action mask\n",
    "                        self.open_leafs.append(child)\n",
    "                while True:\n",
    "                    if node.children[-1].visited==False:\n",
    "                        node.children[-1].visited=True\n",
    "                        node=node.children[-1]\n",
    "                        \n",
    "                        break\n",
    "                    loop+=1\n",
    "                    if loop > len(node.children):\n",
    "                        print(\"all childrens visited; return done equal true\")\n",
    "                        self.done=True\n",
    "            self.done=False\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()\n",
    "init=tree_construct()\n",
    "print(\"solve\")\n",
    "init.solve()\n",
    "print(init.reward)\n",
    "print(init.sol_pathes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray import tune\n",
    "import ray\n",
    "import gym\n",
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper\n",
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper_multi_instances\n",
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper_no_action_mask\n",
    "from src.jss_lite.jss_lite import jss_lite\n",
    "from ray import tune\n",
    "curr_dir='/Users/felix/sciebo/masterarbeit/progra/model-based_rl'\n",
    "inst=curr_dir + '/resources/jsp_instances/standard/la01.txt'\n",
    "instance_list=['/resources/jsp_instances/standard/la01.txt']#,'/resources/jsp_instances/standard/la02.txt','/resources/jsp_instances/standard/la03.txt','/resources/jsp_instances/standard/la04.txt']\n",
    "# Configure the algorithm.\n",
    "# def env_creator(env_config):\n",
    "#     #env=jssp_light_obs_wrapper_multi_instances(instances_list=[inst])\n",
    "#     env=jssp_light_obs_wrapper_no_action_mask(jssp_light_obs_wrapper_multi_instances(instances_list=[inst]))\n",
    "#     #env=jss_lite(instance_path=inst)\n",
    "#     #env=jssp_light_obs_wrapper_no_action_mask(jss_lite(instance_path=inst))\n",
    "#     return env\n",
    "from wrapper.jssp_wrapper_klagenfurt import jssp_klagenfurt_obs_wrapper\n",
    "import JSSEnv\n",
    "# Configure the algorithm.\n",
    "def env_creator(env_config):\n",
    "    import JSSEnv\n",
    "    env = jssp_klagenfurt_obs_wrapper(gym.make('jss-v1', env_config={'instance_path': 'resources/jsp_instances/standard/la01.txt'}))\n",
    "    return env\n",
    "\n",
    "ray.shutdown()\n",
    "# use tune to register the custom environment for the ppo trainer\n",
    "tune.register_env('custom_jssp',env_creator)\n",
    "\n",
    "\n",
    "config = {\n",
    "    # Environment (RLlib understands openAI gym registered strings).\n",
    "    #\"env\": \"CartPole-v1\",\n",
    "    \"env\": \"custom_jssp\",\n",
    "    \"disable_env_checking\":True,\n",
    "\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 4,\n",
    "    # Change this to \"framework: torch\", if you are using PyTorch.\n",
    "    # Also, use \"framework: tf2\" for tf2.x eager execution.\n",
    "    \"framework\": \"tf\",\n",
    "    # Tweak the default model provided automatically by RLlib,\n",
    "    # given the environment's observation- and action spaces.\n",
    "    #\"horizon\":1,\n",
    "    #\"evaluation_duration\":0,\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [64, 64],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    # Set up a separate evaluation worker set for the\n",
    "    # `trainer.evaluate()` call after training (see below).\n",
    "    \"evaluation_num_workers\": 0,\n",
    "    # Only for evaluation runs, render the env.\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": False,\n",
    "    },\n",
    "    \n",
    "}\n",
    "trainer = PPOTrainer(config=config)\n",
    "#trainer.load_checkpoint('/Users/felix/sciebo/masterarbeit/progra/model-based_rl/ppo/checkpoint-4')\n",
    "\n",
    "for _ in range(10):\n",
    "     print(_)\n",
    "     trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#trainer.load_checkpoint('/Users/felix/sciebo/masterarbeit/progra/model-based_rl/ppo/checkpoint-4')\n",
    "import numpy as np\n",
    "#trainer.save_checkpoint(\"ppo\")\n",
    "episode_reward = 0\n",
    "#env=jssp_light_obs_wrapper_multi_instances(instances_list=[inst])\n",
    "#env=jssp_light_obs_wrapper_no_action_mask(jssp_light_obs_wrapper_multi_instances(instances_list=[inst]))\n",
    "env=env_creator(\"s\")\n",
    "done = False\n",
    "obs = env.reset()\n",
    "iterations=0\n",
    "\n",
    "\n",
    "while not done:\n",
    "    policy = trainer.get_policy()\n",
    "    action, _, info = policy.compute_single_action(obs)\n",
    "    logits = info['action_dist_inputs']\n",
    "    action_mask = env.get_legal_actions().astype(np.float32)\n",
    "    action_mask[action_mask==0] = - np.inf\n",
    "    probs=np.multiply(logits,action_mask)\n",
    "    probs[probs==np.inf]=-np.inf\n",
    "    best_action_id = probs.argmax()  # deterministic\n",
    "    best_action_id = logits.argmax()\n",
    "    #best_action_id = trainer.compute_single_action(obs)\n",
    "    obs, reward, done, info = env.step(best_action_id)\n",
    "    episode_reward += reward\n",
    "    iterations += 1\n",
    "\n",
    "    print(best_action_id)\n",
    "env.render()\n",
    "print(iterations)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iterations)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info)\n",
    "print(len(info['action_dist_inputs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper\n",
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper_multi_instances\n",
    "from src.jss_lite.jss_lite import jss_lite\n",
    "curr_dir='/Users/felix/sciebo/masterarbeit/progra/model-based_rl'\n",
    "inst=curr_dir + '/resources/jsp_instances/standard/ft06.txt'\n",
    "instance_list=['/resources/jsp_instances/standard/la01.txt']#,'/resources/jsp_instances/standard/la02.txt','/resources/jsp_instances/standard/la03.txt','/resources/jsp_instances/standard/la04.txt']\n",
    "# Configure the algorithm.\n",
    "def env_creator(env_config):\n",
    "    env=jssp_light_obs_wrapper_multi_instances(instances_list=[inst])\n",
    "    #env=jss_lite(instance_path=inst)\n",
    "    return env\n",
    "# get random actions:\n",
    "reward=0\n",
    "while reward <0.95:\n",
    "#for _ in range(100):\n",
    "    action_list=[]\n",
    "    env = env_creator(\"s\")\n",
    "\n",
    "    obs = env.reset()\n",
    "    # env2 is copy for later going evaluation\n",
    "    counter=0\n",
    "    done = False\n",
    "    t=time.time()\n",
    "    while not done:\n",
    "        legal_action=obs['action_mask']\n",
    "        action=np.random.choice(len(legal_action), 1, p=(legal_action / legal_action.sum()))[0]\n",
    "        action_list.append(action)\n",
    "        #print(action)\n",
    "        #print(obs['action_mask'][action])\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        counter+=1\n",
    "    #print(f\"instance: {env.env.instance} in time: {time.time()-t}s\")\n",
    "    \n",
    "    #env.render()\n",
    "    #env.close()\n",
    "print(counter)\n",
    "print(reward)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x=np.array([2,2,2,2,2,2,2,2,2,2,2,2,4])\n",
    "print(x[x>=0.8*2].argmax())\n",
    "y,=(np.where(x>=2))\n",
    "\n",
    "for i in y:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('cleaninstall')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc7292e1960732d9c4c32925edefe52037f482083a9087ff002fef268bf3b5aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
