{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray import tune\n",
    "import gym\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "train_agent=True\n",
    "instance_path='resources/jsp_instances/standard/ft06.txt'\n",
    "restore_agent= False\n",
    "num_episodes = 10\n",
    "#restore_path= 'training_checkpoints/checkpoints_az_jsslite/checkpoint-6'\n",
    "\n",
    "config = {\n",
    "    # Environment (RLlib understands openAI gym registered strings).\n",
    "    #\"env\": \"CartPole-v1\",\n",
    "    \"env\": \"custom_jssp\",\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 4,\n",
    "    # Change this to \"framework: torch\", if you are using PyTorch.\n",
    "    # Also, use \"framework: tf2\" for tf2.x eager execution.\n",
    "    \"framework\": \"tf\",\n",
    "    # Tweak the default model provided automatically by RLlib,\n",
    "    # given the environment's observation- and action spaces.\n",
    "    #\"horizon\":1,\n",
    "    \"evaluation_duration\":10,\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [256, 256],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    # Set up a separate evaluation worker set for the\n",
    "    # `trainer.evaluate()` call after training (see below).\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    # Only for evaluation runs, render the env.\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": False,\n",
    "    },\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper\n",
    "from src.jss_lite.jss_lite import jss_lite\n",
    "\n",
    "def env_creator(config):\n",
    "    env = jssp_light_obs_wrapper(jss_lite(instance_path=instance_path))\n",
    "    return env\n",
    "\n",
    "\n",
    "# use tune to register the custom environment for the ppo trainer\n",
    "tune.register_env('custom_jssp',env_creator)\n",
    "\n",
    "trainer = PPOTrainer(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "epoch_nr=0\n",
    "\n",
    "for _ in range(60):\n",
    "    epoch_nr+=1\n",
    "    t=time.time()\n",
    "    trainer.train()\n",
    "    episode_reward = 0\n",
    "    env = env_creator(\"config\")\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    iterations=0\n",
    "    while not done:\n",
    "        action = trainer.compute_action(obs)\n",
    "        #print(f\"action: {action}\")\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        #env.render(mode=\"human\")\n",
    "        iterations += 1\n",
    "    #env.render()\n",
    "    #env.close()\n",
    "    #print(f\"{_+60} episode\")\n",
    "    #print(f\"episode reward:{episode_reward}\")\n",
    "    #print(f\"reward:{reward}\")\n",
    "    #print(f\"count iterations:{iterations}\")\n",
    "    #print(f\"trainer trained: {epoch_nr} Epochs in {time.time()-t} seconds for new episode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_reward = 0\n",
    "env = env_creator(\"config\")\n",
    "\n",
    "done = False\n",
    "obs = env.reset()\n",
    "iterations=0\n",
    "while not done:\n",
    "    action = trainer.compute_action(obs)\n",
    "    #print(f\"action: {action}\")\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "    #env.render(mode=\"human\")\n",
    "    iterations += 1\n",
    "env.render()\n",
    "env.close()\n",
    "print(episode_reward)\n",
    "print(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs)\n",
    "print(env.env.current_timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "display(env.env.production_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Example of using training on MBMPO\"\"\"\n",
    "\n",
    "#import argparse\n",
    "#/Users/felix/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/agents/mbmpo/model_ensemble.py\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.mbmpo.model_ensemble import DynamicsEnsembleCustomModel\n",
    "#from ray.rllib.examples.env.mbmpo_env import CartPoleWrapper\n",
    "from ray.rllib.models.catalog import ModelCatalog\n",
    "from ray.rllib.agents.mbmpo.utils import MBMPOExploration \n",
    "from ray.rllib.env.wrappers.model_vector_env import model_vector_env\n",
    "from ray.rllib.utils.deprecation import DEPRECATED_VALUE\n",
    "from ray.rllib.agents.mbmpo.mbmpo import MBMPOTrainer\n",
    "train=True\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\"--num-workers\", default=6, type=int)\n",
    "#parser.add_argument(\"--training-iteration\", default=10000, type=int)\n",
    "#parser.add_argument(\"--ray-num-cpus\", default=7, type=int)\n",
    "#args = parser.parse_args()\n",
    "ray.shutdown()\n",
    "ray.init(num_cpus=7)\n",
    "\n",
    "ModelCatalog.register_custom_model(\"DynamicsEnsembleCustomModel\", DynamicsEnsembleCustomModel)\n",
    "\n",
    "config={\n",
    "    \"env\": env_creator(\"config\"),\n",
    "    \"use_gae\": True,\n",
    "    # GAE(lambda) parameter.\n",
    "    \"lambda\": 1.0,\n",
    "    # Initial coefficient for KL divergence.\n",
    "    \"kl_coeff\": 0.0005,\n",
    "    # Size of batches collected from each worker.\n",
    "    \"rollout_fragment_length\": 200,\n",
    "    # Do create an actual env on the local worker (worker-idx=0).\n",
    "    \"create_env_on_driver\": True,\n",
    "    # Step size of SGD.\n",
    "    \"lr\": 1e-3,\n",
    "    # Coefficient of the value function loss.\n",
    "    \"vf_loss_coeff\": 0.5,\n",
    "    # Coefficient of the entropy regularizer.\n",
    "    \"entropy_coeff\": 0.0,\n",
    "    # PPO clip parameter.\n",
    "    \"clip_param\": 0.5,\n",
    "    # Clip param for the value function. Note that this is sensitive to the\n",
    "    # scale of the rewards. If your expected V is large, increase this.\n",
    "    \"vf_clip_param\": 10.0,\n",
    "    # If specified, clip the global norm of gradients by this amount.\n",
    "    \"grad_clip\": None,\n",
    "    # Target value for KL divergence.\n",
    "    \"kl_target\": 0.01,\n",
    "    # Whether to rollout \"complete_episodes\" or \"truncate_episodes\".\n",
    "    \"batch_mode\": \"complete_episodes\",\n",
    "    # Which observation filter to apply to the observation.\n",
    "    \"observation_filter\": \"NoFilter\",\n",
    "    # Number of Inner adaptation steps for the MAML algorithm.\n",
    "    \"inner_adaptation_steps\": 1,\n",
    "    # Number of MAML steps per meta-update iteration (PPO steps).\n",
    "    \"maml_optimizer_steps\": 8,\n",
    "    # Inner adaptation step size.\n",
    "    \"inner_lr\": 1e-3,\n",
    "    # Horizon of the environment (200 in MB-MPO paper).\n",
    "    \"horizon\": 200,\n",
    "    # Dynamics ensemble hyperparameters.\n",
    "    \"dynamics_model\": {\n",
    "        \"custom_model\": DynamicsEnsembleCustomModel,\n",
    "        # Number of Transition-Dynamics (TD) models in the ensemble.\n",
    "        \"ensemble_size\": 5,\n",
    "        # Hidden layers for each model in the TD-model ensemble.\n",
    "        \"fcnet_hiddens\": [512, 512, 512],\n",
    "        # Model learning rate.\n",
    "        \"lr\": 1e-3,\n",
    "        # Max number of training epochs per MBMPO iter.\n",
    "        \"train_epochs\": 500,\n",
    "        # Model batch size.\n",
    "        \"batch_size\": 500,\n",
    "        # Training/validation split.\n",
    "        \"valid_split_ratio\": 0.2,\n",
    "        # Normalize data (obs, action, and deltas).\n",
    "        \"normalize_data\": True,\n",
    "    },\n",
    "    # Exploration for MB-MPO is based on StochasticSampling, but uses 8000\n",
    "    # random timesteps up-front for worker=0.\n",
    "    \"exploration_config\": {\n",
    "        \"type\": MBMPOExploration,\n",
    "        \"random_timesteps\": 8000,\n",
    "    },\n",
    "    # Workers sample from dynamics models, not from actual envs.\n",
    "    \"custom_vector_env\": model_vector_env,\n",
    "    # How many iterations through MAML per MBMPO iteration.\n",
    "    \"num_maml_steps\": 10,\n",
    "\n",
    "    # Deprecated keys:\n",
    "    # Share layers for value function. If you set this to True, it's important\n",
    "    # to tune vf_loss_coeff.\n",
    "    # Use config.model.vf_share_layers instead.\n",
    "    \"vf_share_layers\": DEPRECATED_VALUE,\n",
    "}\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import sys, os\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:  \n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 12:17:25,776\tWARNING mbmpo.py:351 -- MB-MPO only supported in PyTorch so far! Switching to `framework=torch`.\n",
      "2022-09-28 12:17:25,947\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Env <jssp_light_obs_wrapper<jss_lite instance>> doest not have a `reward()` method, needed for MB-MPO! This `reward()` method should return ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/agents/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 935\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator)\n\u001b[1;32m    936\u001b[0m \u001b[39m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[39m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[39m# parallel to training.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[39m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/agents/trainer.py:1074\u001b[0m, in \u001b[0;36mTrainer._init\u001b[0;34m(self, config, env_creator)\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_init\u001b[39m(\u001b[39mself\u001b[39m, config: TrainerConfigDict, env_creator: EnvCreator) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1074\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/felix/sciebo/masterarbeit/progra/model-based_rl/jssp_light_different_algos.ipynb Zelle 8\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felix/sciebo/masterarbeit/progra/model-based_rl/jssp_light_different_algos.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tune\u001b[39m.\u001b[39mregister_env(\u001b[39m'\u001b[39m\u001b[39mcustom_jssp\u001b[39m\u001b[39m'\u001b[39m,env_creator)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/felix/sciebo/masterarbeit/progra/model-based_rl/jssp_light_different_algos.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer \u001b[39m=\u001b[39m MBMPOTrainer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felix/sciebo/masterarbeit/progra/model-based_rl/jssp_light_different_algos.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felix/sciebo/masterarbeit/progra/model-based_rl/jssp_light_different_algos.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     env\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcustom_jssp\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felix/sciebo/masterarbeit/progra/model-based_rl/jssp_light_different_algos.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m iteration_list\u001b[39m=\u001b[39m[]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felix/sciebo/masterarbeit/progra/model-based_rl/jssp_light_different_algos.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m epoch_nr\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/agents/trainer.py:870\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, config, env, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[39m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \u001b[39m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[39m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {\n\u001b[1;32m    863\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluation\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepisode_reward_max\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     }\n\u001b[1;32m    868\u001b[0m }\n\u001b[0;32m--> 870\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    871\u001b[0m     config, logger_creator, remote_checkpoint_dir, sync_function_tpl\n\u001b[1;32m    872\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/tune/trainable.py:156\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    154\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_ip \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_current_ip()\n\u001b[0;32m--> 156\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig))\n\u001b[1;32m    157\u001b[0m setup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m setup_time \u001b[39m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/agents/trainer.py:950\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[39m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[39m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[39m# parallel to training.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[39m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    944\u001b[0m     \u001b[39m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    945\u001b[0m     \u001b[39m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[39m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    949\u001b[0m     \u001b[39m# has been deprecated.\u001b[39;00m\n\u001b[0;32m--> 950\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m WorkerSet(\n\u001b[1;32m    951\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator,\n\u001b[1;32m    952\u001b[0m         validate_env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_env,\n\u001b[1;32m    953\u001b[0m         policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_policy_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig),\n\u001b[1;32m    954\u001b[0m         trainer_config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[1;32m    955\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnum_workers\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    956\u001b[0m         local_worker\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    957\u001b[0m         logdir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogdir,\n\u001b[1;32m    958\u001b[0m     )\n\u001b[1;32m    959\u001b[0m     \u001b[39m# By default, collect metrics for all remote workers.\u001b[39;00m\n\u001b[1;32m    960\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remote_workers_for_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers\u001b[39m.\u001b[39mremote_workers()\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:170\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    167\u001b[0m     spaces \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m local_worker:\n\u001b[0;32m--> 170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_worker \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_worker(\n\u001b[1;32m    171\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49mRolloutWorker,\n\u001b[1;32m    172\u001b[0m         env_creator\u001b[39m=\u001b[39;49menv_creator,\n\u001b[1;32m    173\u001b[0m         validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[1;32m    174\u001b[0m         policy_cls\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_policy_class,\n\u001b[1;32m    175\u001b[0m         worker_index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m    176\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    177\u001b[0m         config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_local_config,\n\u001b[1;32m    178\u001b[0m         spaces\u001b[39m=\u001b[39;49mspaces,\n\u001b[1;32m    179\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:630\u001b[0m, in \u001b[0;36mWorkerSet._make_worker\u001b[0;34m(self, cls, env_creator, validate_env, policy_cls, worker_index, num_workers, recreated_worker, config, spaces)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     extra_python_environs \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mextra_python_environs_for_worker\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 630\u001b[0m worker \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    631\u001b[0m     env_creator\u001b[39m=\u001b[39;49menv_creator,\n\u001b[1;32m    632\u001b[0m     validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[1;32m    633\u001b[0m     policy_spec\u001b[39m=\u001b[39;49mpolicies,\n\u001b[1;32m    634\u001b[0m     policy_mapping_fn\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mpolicy_mapping_fn\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    635\u001b[0m     policies_to_train\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mpolicies_to_train\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    636\u001b[0m     tf_session_creator\u001b[39m=\u001b[39;49m(session_creator \u001b[39mif\u001b[39;49;00m config[\u001b[39m\"\u001b[39;49m\u001b[39mtf_session_args\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    637\u001b[0m     rollout_fragment_length\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mrollout_fragment_length\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    638\u001b[0m     count_steps_by\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mcount_steps_by\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    639\u001b[0m     batch_mode\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mbatch_mode\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    640\u001b[0m     episode_horizon\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mhorizon\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    641\u001b[0m     preprocessor_pref\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mpreprocessor_pref\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    642\u001b[0m     sample_async\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39msample_async\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    643\u001b[0m     compress_observations\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mcompress_observations\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    644\u001b[0m     num_envs\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnum_envs_per_worker\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    645\u001b[0m     observation_fn\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mobservation_fn\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    646\u001b[0m     observation_filter\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mobservation_filter\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    647\u001b[0m     clip_rewards\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mclip_rewards\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    648\u001b[0m     normalize_actions\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnormalize_actions\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    649\u001b[0m     clip_actions\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mclip_actions\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    650\u001b[0m     env_config\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39menv_config\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    651\u001b[0m     policy_config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    652\u001b[0m     worker_index\u001b[39m=\u001b[39;49mworker_index,\n\u001b[1;32m    653\u001b[0m     num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    654\u001b[0m     recreated_worker\u001b[39m=\u001b[39;49mrecreated_worker,\n\u001b[1;32m    655\u001b[0m     record_env\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mrecord_env\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    656\u001b[0m     log_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_logdir,\n\u001b[1;32m    657\u001b[0m     log_level\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mlog_level\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    658\u001b[0m     callbacks\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    659\u001b[0m     input_creator\u001b[39m=\u001b[39;49minput_creator,\n\u001b[1;32m    660\u001b[0m     input_evaluation\u001b[39m=\u001b[39;49minput_evaluation,\n\u001b[1;32m    661\u001b[0m     output_creator\u001b[39m=\u001b[39;49moutput_creator,\n\u001b[1;32m    662\u001b[0m     remote_worker_envs\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mremote_worker_envs\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    663\u001b[0m     remote_env_batch_wait_ms\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mremote_env_batch_wait_ms\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    664\u001b[0m     soft_horizon\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39msoft_horizon\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    665\u001b[0m     no_done_at_end\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mno_done_at_end\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    666\u001b[0m     seed\u001b[39m=\u001b[39;49m(config[\u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m+\u001b[39;49m worker_index)\n\u001b[1;32m    667\u001b[0m     \u001b[39mif\u001b[39;49;00m config[\u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    668\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    669\u001b[0m     fake_sampler\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mfake_sampler\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    670\u001b[0m     extra_python_environs\u001b[39m=\u001b[39;49mextra_python_environs,\n\u001b[1;32m    671\u001b[0m     spaces\u001b[39m=\u001b[39;49mspaces,\n\u001b[1;32m    672\u001b[0m     disable_env_checking\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mdisable_env_checking\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    673\u001b[0m )\n\u001b[1;32m    675\u001b[0m \u001b[39mreturn\u001b[39;00m worker\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py:515\u001b[0m, in \u001b[0;36mRolloutWorker.__init__\u001b[0;34m(self, env_creator, validate_env, policy_spec, policy_mapping_fn, policies_to_train, tf_session_creator, rollout_fragment_length, count_steps_by, batch_mode, episode_horizon, preprocessor_pref, sample_async, compress_observations, num_envs, observation_fn, observation_filter, clip_rewards, normalize_actions, clip_actions, env_config, model_config, policy_config, worker_index, num_workers, recreated_worker, record_env, log_dir, log_level, callbacks, input_creator, input_evaluation, output_creator, remote_worker_envs, remote_env_batch_wait_ms, soft_horizon, no_done_at_end, seed, extra_python_environs, fake_sampler, spaces, policy, monitor_path, disable_env_checking)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39m# Custom validation function given, typically a function attribute of the\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[39m# algorithm trainer.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[39mif\u001b[39;00m validate_env \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m     validate_env(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_context)\n\u001b[1;32m    516\u001b[0m \u001b[39m# We can't auto-wrap a BaseEnv.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, (BaseEnv, ray\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39mActorHandle)):\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/agents/mbmpo/mbmpo.py:468\u001b[0m, in \u001b[0;36mMBMPOTrainer.validate_env\u001b[0;34m(env, env_context)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39m\"\"\"Validates the local_worker's env object (after creation).\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \n\u001b[1;32m    459\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[39m    ValueError: In case something is wrong with the config.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(env, \u001b[39m\"\u001b[39m\u001b[39mreward\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(env\u001b[39m.\u001b[39mreward):\n\u001b[0;32m--> 468\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    469\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEnv \u001b[39m\u001b[39m{\u001b[39;00menv\u001b[39m}\u001b[39;00m\u001b[39m doest not have a `reward()` method, needed for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    470\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMB-MPO! This `reward()` method should return \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    471\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Env <jssp_light_obs_wrapper<jss_lite instance>> doest not have a `reward()` method, needed for MB-MPO! This `reward()` method should return "
     ]
    }
   ],
   "source": [
    "tune.register_env('custom_jssp',env_creator)\n",
    "trainer = MBMPOTrainer(\n",
    "    config=config,\n",
    "    env='custom_jssp')\n",
    "\n",
    "\n",
    "iteration_list=[]\n",
    "epoch_nr=0\n",
    "for _ in range(5):\n",
    "    epoch_nr+=1\n",
    "    with suppress_stdout():\n",
    "        trainer.train()\n",
    "    print(f\"trainer trained: {epoch_nr} Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env_creator(\"config\")\n",
    "#env = gym.make('LunarL')\n",
    "#env.render()\n",
    "# run until episode ends\n",
    "# list that lists all actions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('customjssp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Mar 31 2022, 03:38:35) [Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7344f7b5995cbf62a990a56ee6eec8bd53f41a3aff848cd18f1feb05905aa9e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
