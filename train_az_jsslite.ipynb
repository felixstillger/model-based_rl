{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/agents/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 935\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator)\n\u001b[1;32m    936\u001b[0m \u001b[39m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[39m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[39m# parallel to training.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[39m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/agents/trainer.py:1074\u001b[0m, in \u001b[0;36mTrainer._init\u001b[0;34m(self, config, env_creator)\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_init\u001b[39m(\u001b[39mself\u001b[39m, config: TrainerConfigDict, env_creator: EnvCreator) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1074\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/felix/sciebo/masterarbeit/progra/model-based_rl/train_az_jsslite.ipynb Zelle 1\u001b[0m in \u001b[0;36m<cell line: 73>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felix/sciebo/masterarbeit/progra/model-based_rl/train_az_jsslite.ipynb#W0sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# use tune to register the custom environment for the ppo trainer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felix/sciebo/masterarbeit/progra/model-based_rl/train_az_jsslite.ipynb#W0sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m tune\u001b[39m.\u001b[39mregister_env(\u001b[39m'\u001b[39m\u001b[39mcustom_jssp\u001b[39m\u001b[39m'\u001b[39m,env_creator)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/felix/sciebo/masterarbeit/progra/model-based_rl/train_az_jsslite.ipynb#W0sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m agent \u001b[39m=\u001b[39m AlphaZeroTrainer( config\u001b[39m=\u001b[39;49mconfig, env\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcustom_jssp\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/agents/trainer.py:870\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, config, env, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[39m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \u001b[39m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[39m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {\n\u001b[1;32m    863\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluation\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepisode_reward_max\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     }\n\u001b[1;32m    868\u001b[0m }\n\u001b[0;32m--> 870\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    871\u001b[0m     config, logger_creator, remote_checkpoint_dir, sync_function_tpl\n\u001b[1;32m    872\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/tune/trainable.py:156\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    154\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_ip \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_current_ip()\n\u001b[0;32m--> 156\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig))\n\u001b[1;32m    157\u001b[0m setup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m setup_time \u001b[39m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/agents/trainer.py:950\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[39m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[39m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[39m# parallel to training.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[39m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    944\u001b[0m     \u001b[39m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    945\u001b[0m     \u001b[39m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[39m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    949\u001b[0m     \u001b[39m# has been deprecated.\u001b[39;00m\n\u001b[0;32m--> 950\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m WorkerSet(\n\u001b[1;32m    951\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator,\n\u001b[1;32m    952\u001b[0m         validate_env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_env,\n\u001b[1;32m    953\u001b[0m         policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_policy_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig),\n\u001b[1;32m    954\u001b[0m         trainer_config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[1;32m    955\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnum_workers\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    956\u001b[0m         local_worker\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    957\u001b[0m         logdir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogdir,\n\u001b[1;32m    958\u001b[0m     )\n\u001b[1;32m    959\u001b[0m     \u001b[39m# By default, collect metrics for all remote workers.\u001b[39;00m\n\u001b[1;32m    960\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remote_workers_for_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers\u001b[39m.\u001b[39mremote_workers()\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:170\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    167\u001b[0m     spaces \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m local_worker:\n\u001b[0;32m--> 170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_worker \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_worker(\n\u001b[1;32m    171\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49mRolloutWorker,\n\u001b[1;32m    172\u001b[0m         env_creator\u001b[39m=\u001b[39;49menv_creator,\n\u001b[1;32m    173\u001b[0m         validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[1;32m    174\u001b[0m         policy_cls\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_policy_class,\n\u001b[1;32m    175\u001b[0m         worker_index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m    176\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    177\u001b[0m         config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_local_config,\n\u001b[1;32m    178\u001b[0m         spaces\u001b[39m=\u001b[39;49mspaces,\n\u001b[1;32m    179\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:630\u001b[0m, in \u001b[0;36mWorkerSet._make_worker\u001b[0;34m(self, cls, env_creator, validate_env, policy_cls, worker_index, num_workers, recreated_worker, config, spaces)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     extra_python_environs \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mextra_python_environs_for_worker\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 630\u001b[0m worker \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    631\u001b[0m     env_creator\u001b[39m=\u001b[39;49menv_creator,\n\u001b[1;32m    632\u001b[0m     validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[1;32m    633\u001b[0m     policy_spec\u001b[39m=\u001b[39;49mpolicies,\n\u001b[1;32m    634\u001b[0m     policy_mapping_fn\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mpolicy_mapping_fn\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    635\u001b[0m     policies_to_train\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mpolicies_to_train\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    636\u001b[0m     tf_session_creator\u001b[39m=\u001b[39;49m(session_creator \u001b[39mif\u001b[39;49;00m config[\u001b[39m\"\u001b[39;49m\u001b[39mtf_session_args\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    637\u001b[0m     rollout_fragment_length\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mrollout_fragment_length\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    638\u001b[0m     count_steps_by\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mcount_steps_by\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    639\u001b[0m     batch_mode\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mbatch_mode\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    640\u001b[0m     episode_horizon\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mhorizon\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    641\u001b[0m     preprocessor_pref\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mpreprocessor_pref\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    642\u001b[0m     sample_async\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39msample_async\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    643\u001b[0m     compress_observations\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mcompress_observations\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    644\u001b[0m     num_envs\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnum_envs_per_worker\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    645\u001b[0m     observation_fn\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmultiagent\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mobservation_fn\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    646\u001b[0m     observation_filter\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mobservation_filter\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    647\u001b[0m     clip_rewards\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mclip_rewards\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    648\u001b[0m     normalize_actions\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnormalize_actions\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    649\u001b[0m     clip_actions\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mclip_actions\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    650\u001b[0m     env_config\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39menv_config\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    651\u001b[0m     policy_config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    652\u001b[0m     worker_index\u001b[39m=\u001b[39;49mworker_index,\n\u001b[1;32m    653\u001b[0m     num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    654\u001b[0m     recreated_worker\u001b[39m=\u001b[39;49mrecreated_worker,\n\u001b[1;32m    655\u001b[0m     record_env\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mrecord_env\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    656\u001b[0m     log_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_logdir,\n\u001b[1;32m    657\u001b[0m     log_level\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mlog_level\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    658\u001b[0m     callbacks\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    659\u001b[0m     input_creator\u001b[39m=\u001b[39;49minput_creator,\n\u001b[1;32m    660\u001b[0m     input_evaluation\u001b[39m=\u001b[39;49minput_evaluation,\n\u001b[1;32m    661\u001b[0m     output_creator\u001b[39m=\u001b[39;49moutput_creator,\n\u001b[1;32m    662\u001b[0m     remote_worker_envs\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mremote_worker_envs\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    663\u001b[0m     remote_env_batch_wait_ms\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mremote_env_batch_wait_ms\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    664\u001b[0m     soft_horizon\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39msoft_horizon\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    665\u001b[0m     no_done_at_end\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mno_done_at_end\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    666\u001b[0m     seed\u001b[39m=\u001b[39;49m(config[\u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m+\u001b[39;49m worker_index)\n\u001b[1;32m    667\u001b[0m     \u001b[39mif\u001b[39;49;00m config[\u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    668\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    669\u001b[0m     fake_sampler\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mfake_sampler\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    670\u001b[0m     extra_python_environs\u001b[39m=\u001b[39;49mextra_python_environs,\n\u001b[1;32m    671\u001b[0m     spaces\u001b[39m=\u001b[39;49mspaces,\n\u001b[1;32m    672\u001b[0m     disable_env_checking\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mdisable_env_checking\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    673\u001b[0m )\n\u001b[1;32m    675\u001b[0m \u001b[39mreturn\u001b[39;00m worker\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py:630\u001b[0m, in \u001b[0;36mRolloutWorker.__init__\u001b[0;34m(self, env_creator, validate_env, policy_spec, policy_mapping_fn, policies_to_train, tf_session_creator, rollout_fragment_length, count_steps_by, batch_mode, episode_horizon, preprocessor_pref, sample_async, compress_observations, num_envs, observation_fn, observation_filter, clip_rewards, normalize_actions, clip_actions, env_config, model_config, policy_config, worker_index, num_workers, recreated_worker, record_env, log_dir, log_level, callbacks, input_creator, input_evaluation, output_creator, remote_worker_envs, remote_env_batch_wait_ms, soft_horizon, no_done_at_end, seed, extra_python_environs, fake_sampler, spaces, policy, monitor_path, disable_env_checking)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[1;32m    618\u001b[0m     ray\u001b[39m.\u001b[39mis_initialized()\n\u001b[1;32m    619\u001b[0m     \u001b[39mand\u001b[39;00m ray\u001b[39m.\u001b[39mworker\u001b[39m.\u001b[39m_mode() \u001b[39m==\u001b[39m ray\u001b[39m.\u001b[39mworker\u001b[39m.\u001b[39mLOCAL_MODE\n\u001b[1;32m    620\u001b[0m     \u001b[39mand\u001b[39;00m num_gpus \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    621\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m policy_config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_fake_gpus\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    622\u001b[0m ):\n\u001b[1;32m    623\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    624\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are running ray with `local_mode=True`, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    625\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconfigured \u001b[39m\u001b[39m{\u001b[39;00mnum_gpus\u001b[39m}\u001b[39;00m\u001b[39m GPUs to be used! In local mode, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    626\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPolicies are placed on the CPU and the `num_gpus` setting \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    627\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis ignored.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    628\u001b[0m     )\n\u001b[0;32m--> 630\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_policy_map(\n\u001b[1;32m    631\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_dict,\n\u001b[1;32m    632\u001b[0m     policy_config,\n\u001b[1;32m    633\u001b[0m     session_creator\u001b[39m=\u001b[39;49mtf_session_creator,\n\u001b[1;32m    634\u001b[0m     seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m    635\u001b[0m )\n\u001b[1;32m    637\u001b[0m \u001b[39m# Update Policy's view requirements from Model, only if Policy directly\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[39m# inherited from base `Policy` class. At this point here, the Policy\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[39m# must have it's Model (if any) defined and ready to output an initial\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[39m# state.\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[39mfor\u001b[39;00m pol \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_map\u001b[39m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py:1788\u001b[0m, in \u001b[0;36mRolloutWorker._build_policy_map\u001b[0;34m(self, policy_dict, policy_config, session_creator, seed)\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocessors[name] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m     \u001b[39m# Create the actual policy object.\u001b[39;00m\n\u001b[0;32m-> 1788\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_map\u001b[39m.\u001b[39;49mcreate_policy(\n\u001b[1;32m   1789\u001b[0m         name, orig_cls, obs_space, act_space, conf, merged_conf\n\u001b[1;32m   1790\u001b[0m     )\n\u001b[1;32m   1792\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworker_index \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1793\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBuilt policy map: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_map\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py:152\u001b[0m, in \u001b[0;36mPolicyMap.create_policy\u001b[0;34m(self, policy_id, policy_cls, observation_space, action_space, config_override, merged_config)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39m# Non-tf: No graph, no session.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     class_ \u001b[39m=\u001b[39m policy_cls\n\u001b[0;32m--> 152\u001b[0m     \u001b[39mself\u001b[39m[policy_id] \u001b[39m=\u001b[39m class_(observation_space, action_space, merged_config)\n\u001b[1;32m    154\u001b[0m \u001b[39m# Store spec (class, obs-space, act-space, and config overrides) such\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39m# that the map will be able to reproduce on-the-fly added policies\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39m# from disk.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_specs[policy_id] \u001b[39m=\u001b[39m PolicySpec(\n\u001b[1;32m    158\u001b[0m     policy_class\u001b[39m=\u001b[39mpolicy_cls,\n\u001b[1;32m    159\u001b[0m     observation_space\u001b[39m=\u001b[39mobservation_space,\n\u001b[1;32m    160\u001b[0m     action_space\u001b[39m=\u001b[39maction_space,\n\u001b[1;32m    161\u001b[0m     config\u001b[39m=\u001b[39mconfig_override,\n\u001b[1;32m    162\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/contrib/alpha_zero/core/alpha_zero_trainer.py:172\u001b[0m, in \u001b[0;36mAlphaZeroPolicyWrapperClass.__init__\u001b[0;34m(self, obs_space, action_space, config)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmcts_creator\u001b[39m():\n\u001b[1;32m    170\u001b[0m     \u001b[39mreturn\u001b[39;00m MCTS(model, config[\u001b[39m\"\u001b[39m\u001b[39mmcts_config\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 172\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    173\u001b[0m     obs_space,\n\u001b[1;32m    174\u001b[0m     action_space,\n\u001b[1;32m    175\u001b[0m     config,\n\u001b[1;32m    176\u001b[0m     model,\n\u001b[1;32m    177\u001b[0m     alpha_zero_loss,\n\u001b[1;32m    178\u001b[0m     TorchCategorical,\n\u001b[1;32m    179\u001b[0m     mcts_creator,\n\u001b[1;32m    180\u001b[0m     _env_creator,\n\u001b[1;32m    181\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/contrib/alpha_zero/core/alpha_zero_policy.py:38\u001b[0m, in \u001b[0;36mAlphaZeroPolicy.__init__\u001b[0;34m(self, observation_space, action_space, config, model, loss, action_distribution_class, mcts_creator, env_creator, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv_creator \u001b[39m=\u001b[39m env_creator\n\u001b[1;32m     37\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmcts \u001b[39m=\u001b[39m mcts_creator()\n\u001b[0;32m---> 38\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator()\n\u001b[1;32m     39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset()\n\u001b[1;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_space \u001b[39m=\u001b[39m observation_space\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/contrib/alpha_zero/core/alpha_zero_trainer.py:162\u001b[0m, in \u001b[0;36mAlphaZeroPolicyWrapperClass.__init__.<locals>._env_creator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_env_creator\u001b[39m():\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m env_cls(config[\u001b[39m\"\u001b[39;49m\u001b[39menv_config\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/contrib/alpha_zero/core/ranked_rewards.py:43\u001b[0m, in \u001b[0;36mget_r2_env_wrapper.<locals>.RankedRewardsEnvWrapper.__init__\u001b[0;34m(self, env_config)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mr2_buffer \u001b[39m=\u001b[39m RankedRewardsBuffer(max_buffer_length, percentile)\n\u001b[1;32m     42\u001b[0m \u001b[39mif\u001b[39;00m r2_config[\u001b[39m\"\u001b[39m\u001b[39minitialize_buffer\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize_buffer(r2_config[\u001b[39m\"\u001b[39;49m\u001b[39mnum_init_rewards\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/miniconda3/envs/customjssp/lib/python3.10/site-packages/ray/rllib/contrib/alpha_zero/core/ranked_rewards.py:54\u001b[0m, in \u001b[0;36mget_r2_env_wrapper.<locals>.RankedRewardsEnvWrapper._initialize_buffer\u001b[0;34m(self, num_init_rewards)\u001b[0m\n\u001b[1;32m     52\u001b[0m     probs \u001b[39m=\u001b[39m mask \u001b[39m/\u001b[39m mask\u001b[39m.\u001b[39msum()\n\u001b[1;32m     53\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(np\u001b[39m.\u001b[39marange(mask\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]), p\u001b[39m=\u001b[39mprobs)\n\u001b[0;32m---> 54\u001b[0m     obs, reward, done, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mr2_buffer\u001b[39m.\u001b[39madd_reward(reward)\n",
      "File \u001b[0;32m~/sciebo/masterarbeit/progra/model-based_rl/wrapper/jssplight_wrapper.py:77\u001b[0m, in \u001b[0;36mjssp_light_obs_wrapper_multi_instances.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m,action):\n\u001b[0;32m---> 77\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mobs\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39masarray([\u001b[39m*\u001b[39mobs[\u001b[39m'\u001b[39m\u001b[39mobs\u001b[39m\u001b[39m'\u001b[39m],\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_padding_size,dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64)]), \u001b[39m\"\u001b[39m\u001b[39maction_mask\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39masarray([\u001b[39m*\u001b[39mobs[\u001b[39m'\u001b[39m\u001b[39maction_mask\u001b[39m\u001b[39m'\u001b[39m],\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_mask_padding_size,dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32)])}, reward, done, info\n",
      "File \u001b[0;32m~/sciebo/masterarbeit/progra/model-based_rl/src/jss_lite/jss_lite.py:273\u001b[0m, in \u001b[0;36mjss_lite.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[39m    for i in range(self.n_jobs):\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m \n\u001b[1;32m    267\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39m#update mask in observation\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[1;32m    272\u001b[0m \u001b[39m#self.observation[:,0]=self.get_legal_actions(self.observation)\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation[:,\u001b[39m0\u001b[39m]\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_legal_actions()\n\u001b[1;32m    274\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation[:,\u001b[39m0\u001b[39m]:\n\u001b[1;32m    275\u001b[0m     \u001b[39m#print(\"go to next timestemp possible\")\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     \u001b[39m#implement to jump to next timestemp\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     \u001b[39m#check if done? todo: add aditional checks like are all tasks done and satisfied or just problems with timestemp?\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimestemp_list:\n\u001b[1;32m    279\u001b[0m         \u001b[39m#print(\"finished\")\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         \u001b[39m# this is reward to mini makespan: todo:implement utilisation etc.\u001b[39;00m\n",
      "File \u001b[0;32m~/sciebo/masterarbeit/progra/model-based_rl/src/jss_lite/jss_lite.py:408\u001b[0m, in \u001b[0;36mjss_lite.get_legal_actions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m j\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(j)\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcount_finished_tasks_job_matrix[j] \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_tasks\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m    407\u001b[0m     \u001b[39m## todo comment and rework\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjob_machine_matrix[i,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcount_finished_tasks_job_matrix[i]]\u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjob_machine_matrix[j,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcount_finished_tasks_job_matrix[j]\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[1;32m    409\u001b[0m         \u001b[39m#True\u001b[39;00m\n\u001b[1;32m    410\u001b[0m         \u001b[39mif\u001b[39;00m j \u001b[39m!=\u001b[39m i:\n\u001b[1;32m    411\u001b[0m             action_mask[i\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs]\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m                             \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ray import rllib, tune\n",
    "from ray.rllib.contrib.alpha_zero.core.alpha_zero_trainer import AlphaZeroTrainer\n",
    "from ray.rllib.policy.sample_batch import DEFAULT_POLICY_ID\n",
    "from ray.rllib.policy.policy_map import PolicyMap\n",
    "from ray.rllib.evaluation.episode import MultiAgentEpisode\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.contrib.alpha_zero.models.custom_torch_models import DenseModel\n",
    "from ray.rllib.models.catalog import ModelCatalog\n",
    "import gym\n",
    "from src.jss_lite.jss_lite import jss_lite\n",
    "ModelCatalog.register_custom_model(\"dense_model\", DenseModel)\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "instance_list=['resources/jsp_instances/standard/ft06.txt','resources/jsp_instances/standard/ft10.txt','resources/jsp_instances/standard/ft20.txt','resources/jsp_instances/standard/abz5.txt','resources/jsp_instances/standard/abz6.txt','resources/jsp_instances/standard/abz7.txt']\n",
    "#instance_list=['resources/jsp_instances/standard/ft06.txt']\n",
    "\n",
    "train_agent=True\n",
    "instance_path='resources/jsp_instances/standard/ft06.txt'\n",
    "restore_agent= False\n",
    "num_episodes = 1\n",
    "restore_path= 'published_checkpoints/checkpoints_az_jsslite/checkpoint-255'\n",
    "config = {\n",
    "    \"framework\": \"torch\",\n",
    "    \"disable_env_checking\":True,\n",
    "    \"num_workers\"       : 6,\n",
    "    \"rollout_fragment_length\": 50,\n",
    "    \"train_batch_size\"  : 500,\n",
    "    \"sgd_minibatch_size\": 64,\n",
    "    \"lr\"                : 0.0001,\n",
    "    #\"horizon\"           : 600,\n",
    "    #\"soft_horizon\"      : True,\n",
    "    \"num_sgd_iter\"      : 1,\n",
    "    #\"horizon\"           : 100,\n",
    "    \"mcts_config\"       : {\n",
    "        \"puct_coefficient\"   : 1.5,\n",
    "        \"num_simulations\"    : 100,\n",
    "        \"temperature\"        : 1.0,\n",
    "        \"dirichlet_epsilon\"  : 0.20,\n",
    "        \"dirichlet_noise\"    : 0.03,\n",
    "        \"argmax_tree_policy\" : False,\n",
    "        \"add_dirichlet_noise\": False,\n",
    "    },\n",
    "    \"ranked_rewards\"    : {\n",
    "        \"enable\": True,\n",
    "    },\n",
    "    \"model\"             : {\n",
    "        \"custom_model\": \"dense_model\",\n",
    "\n",
    "    },\n",
    "}\n",
    "\n",
    "# def env_creator(env_config):\n",
    "#     env = jss_lite(instance_path='resources/jsp_instances/standard/ft06.txt')\n",
    "#     return env\n",
    "\n",
    "#from wrapper.jssplight_wrapper import jssp_light_obs_wrapper\n",
    "from wrapper.jssplight_wrapper import jssp_light_obs_wrapper_multi_instances\n",
    "\n",
    "def env_creator(config):\n",
    "    #env = jssp_light_obs_wrapper(jss_lite(instance_path=instance_path))\n",
    "    env=jssp_light_obs_wrapper_multi_instances(instances_list=instance_list)\n",
    "    return env\n",
    "\n",
    "ModelCatalog.register_custom_model(\"dense_model\", DenseModel)    \n",
    "\n",
    "\n",
    "\n",
    "print(\"register tune\")\n",
    "# use tune to register the custom environment for the ppo trainer\n",
    "tune.register_env('custom_jssp',env_creator)\n",
    "print(\"init agent\")\n",
    "\n",
    "agent = AlphaZeroTrainer( config=config, env='custom_jssp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "if train_agent:\n",
    "    # checkpoint_path = analysis.get_last_checkpoint() or args.checkpoint\n",
    "    ## use string number to restore pre trained agent\n",
    "    # nr_restore=\"10\"\n",
    "    #checkpoint_path=f'checkpoints_az/rllib_checkpoint{nr_restore}/checkpoint_{nr_restore.zfill(6)}/checkpoint-{nr_restore}'\n",
    "    #agent.load_checkpoint(\"training_checkpoints/checkpoints_az_jsslite/checkpoint-5\")\n",
    "    #print(\"awd\")\n",
    "    #agent.restore(\"checkpoints_az/rllib_checkpoint1/checkpoint_000001/checkpoint-1\")\n",
    "    #agent.load_checkpoint(\"published_checkpoints/az_taxi/checkpoint-34\")\n",
    "    print(\"start training\")\n",
    "    for _ in range(0,num_episodes):\n",
    "        t=time.time()\n",
    "        agent.train()\n",
    "        print(f\"training iteration {_} finished after {time.time()-t} seconds\")\n",
    "        agent.save_checkpoint(f\"training_checkpoints/checkpoints_az_jsslite\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune.run(\n",
    "    \"contrib/AlphaZero\",\n",
    "    stop={\"training_iteration\": 10},\n",
    "    max_failures=0,\n",
    "    config={\n",
    "        \"env\": 'custom_jssp',\n",
    "        \"num_workers\": 3,\n",
    "        \"rollout_fragment_length\": 50,\n",
    "        \"train_batch_size\": 50,\n",
    "        \"sgd_minibatch_size\": 32,\n",
    "        \"lr\": 1e-4,\n",
    "        \"num_sgd_iter\": 1,\n",
    "        \"mcts_config\": {\n",
    "            \"puct_coefficient\": 1.5,\n",
    "            \"num_simulations\": 100,\n",
    "            \"temperature\": 1.0,\n",
    "            \"dirichlet_epsilon\": 0.20,\n",
    "            \"dirichlet_noise\": 0.03,\n",
    "            \"argmax_tree_policy\": False,\n",
    "            \"add_dirichlet_noise\": True,\n",
    "        },\n",
    "        \"ranked_rewards\": {\n",
    "            \"enable\": True,\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"custom_model\": \"dense_model\",\n",
    "        },\n",
    "        \"evaluation_interval\": 1,\n",
    "        \"evaluation_config\": {\n",
    "            \"render_env\": True,\n",
    "            \"mcts_config\": {\n",
    "                \"argmax_tree_policy\": True,\n",
    "                \"add_dirichlet_noise\": False,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restore_agent:\n",
    "\n",
    "    agent.load_checkpoint(restore_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = env_creator(\"setting\")\n",
    "# config.update(\n",
    "#     simple_optimizer=True,\n",
    "#     num_workers=0,\n",
    "#     train_batch_size=0,\n",
    "#     rollout_fragment_length=0,\n",
    "#     timesteps_per_iteration=0,\n",
    "#     evaluation_interval=1,\n",
    "#     # evaluation_num_workers=...,\n",
    "#     # evaluation_config=dict(explore=False),\n",
    "#     # evaluation_num_episodes=...,\n",
    "# )\n",
    "\n",
    "# results = tune.run(\n",
    "#     agent,\n",
    "#     config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.load_checkpoint(\"training_checkpoints/checkpoints_az_jsslite/checkpoint-5\")\n",
    "for _ in range(10):\n",
    "    import time\n",
    "    length_list=[]\n",
    "    reward_list=[]\n",
    "    for _ in range(1):\n",
    "        policy = agent.get_policy(DEFAULT_POLICY_ID)\n",
    "        action_list=[]\n",
    "        env = env_creator(\"s\")\n",
    "\n",
    "        obs = env.reset()\n",
    "        # env2 is copy for later going evaluation\n",
    "        env2=deepcopy(env)\n",
    "\n",
    "        episode = MultiAgentEpisode(\n",
    "            PolicyMap(0,0),\n",
    "            lambda _, __: DEFAULT_POLICY_ID,\n",
    "            lambda: None,\n",
    "            lambda _: None,\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        episode.user_data['initial_state'] = env.get_state()\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, _, _ = policy.compute_single_action(obs, episode=episode)\n",
    "            action_list.append(action)\n",
    "            #print(action_dic[action])\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            #print(obs)\n",
    "            #env.render(render_mode='human')\n",
    "            #time.sleep(0.1)\n",
    "            episode.length += 1\n",
    "\n",
    "        length_list.append(episode.length)\n",
    "        reward_list.append(reward)\n",
    "        print(reward)\n",
    "        env.render()\n",
    "        #env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.env.current_timestep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.env.done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.env.production_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_1=env.reset()\n",
    "#print(obs_1)\n",
    "\n",
    "env_1=env_creator(\"a\")\n",
    "env_2=env_creator(\"b\")\n",
    "state_1=env_1.reset()\n",
    "state_2=env_2.reset()\n",
    "\n",
    "for a in a_list:\n",
    "    state_1, reward_1, done_1, info_1 = env_1.step(a)\n",
    "    state_2, reward_2, done_2, info_2 = env_2.step(a)\n",
    "\n",
    "    if np.array_equal(state_1['obs'],state_2['obs'])==False :\n",
    "        print(\"error obs\")\n",
    "        print(state_1)\n",
    "        print(state_2)\n",
    "    if np.array_equal(state_1['action_mask'],state_2['action_mask'])==False :\n",
    "        print(\"error mask\")\n",
    "        print(state_1)\n",
    "        print(state_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    \n",
    "    agent.load_checkpoint('training_checkpoints/checkpoints_az_jsslite/checkpoint-'+str(i+1))\n",
    "    policy = agent.get_policy(DEFAULT_POLICY_ID)\n",
    "    action_list=[]\n",
    "    env = env_creator(\"s\")\n",
    "\n",
    "    obs = env.reset()\n",
    "    # env2 is copy for later going evaluation\n",
    "    #env2=deepcopy(env)\n",
    "\n",
    "    episode = MultiAgentEpisode(\n",
    "        PolicyMap(0,0),\n",
    "        lambda _, __: DEFAULT_POLICY_ID,\n",
    "        lambda: None,\n",
    "        lambda _: None,\n",
    "        0,\n",
    "    )\n",
    "\n",
    "    episode.user_data['initial_state'] = env.get_state()\n",
    "\n",
    "    done = False\n",
    "    steps=0\n",
    "    t=time.time()\n",
    "    while not done:\n",
    "        action, _, _ = policy.compute_single_action(obs, episode=episode)\n",
    "        action_list.append(action)\n",
    "        #print(action_dic[action])\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        #print(obs)\n",
    "        #env.render(render_mode='human')\n",
    "        #time.sleep(0.1)\n",
    "        steps+=1\n",
    "    print(f\"checkpoint {i} got reward {reward} in {steps} steps and time: {time.time()-t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.env.job_tasklength_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reward)\n",
    "print(env.env.count_finished_tasks_job_matrix)\n",
    "env.render()\n",
    "env.render(y_bar=\"Machine\",x_bar=\"Job\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()\n",
    "#env.render(y_bar=\"Machine\",x_bar=\"Job\")\n",
    "print(done)\n",
    "print(reward)\n",
    "print(env.invalid_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "from random import randrange\n",
    "a_list=[]\n",
    "for _ in range(30):\n",
    "    state=env.reset()\n",
    "    done=False\n",
    "    i=0\n",
    "    reward=0\n",
    "    #print(state['obs'].shape)\n",
    "    #for i in range(150):\n",
    "    while not done:\n",
    "        i+=1\n",
    "        legal_action=state['action_mask']\n",
    "        #print(legal_action)\n",
    "        action=np.random.choice(len(legal_action), 1, p=(legal_action / legal_action.sum()))[0]\n",
    "        a_list.append(action)\n",
    "        #action=randrange(env.action_space.n/2)\n",
    "        #print(action)\n",
    "        token=True\n",
    "        #print(\"before:\")\n",
    "        #env.render(start_count=1,x_bar=\"Job\",y_bar=\"Machine\")\n",
    "        state, reward, done, info=env.step(action)\n",
    "    #env.render()\n",
    "    print(reward)\n",
    "    \n",
    "    if reward > -68:\n",
    "        print(env.env.get_legal_actions(\"yy\"))\n",
    "        print(env.done)\n",
    "        print(env.env.done)\n",
    "        print(env.env.production_list)\n",
    "        env.env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.env.get_legal_actions(\"a\"))\n",
    "print(env.env.current_timestep)\n",
    "print(env.env.current_machines_status)\n",
    "print(env.env.processed_and_max_time_job_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.env.get_legal_actions(\"a\"))\n",
    "print(env.env.blocked_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.env.blocked_actions)\n",
    "print(env.env.get_legal_actions(\"obs\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[[1,2,None],[1,2,3]]\n",
    "if any(x==None for x in x):\n",
    "        print(True)\n",
    "for row in x:\n",
    "    if None in row:\n",
    "        #print(True)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.env.current_machines_status)\n",
    "print(env.env.get_legal_actions(\"stat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('customjssp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7344f7b5995cbf62a990a56ee6eec8bd53f41a3aff848cd18f1feb05905aa9e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
